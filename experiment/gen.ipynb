{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import pandas as pd\n",
    "import traceback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Directory created at C:\\WINDOWS\\TEMP\\tmpcmts5mn7\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "# Path to create\n",
    "directory_path = r\"C:\\WINDOWS\\TEMP\\tmpcmts5mn7\"\n",
    "\n",
    "# Create the directory\n",
    "os.makedirs(directory_path, exist_ok=True)\n",
    "print(f\"Directory created at {directory_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()  # take environment variables from .env."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chat_models import ChatOpenAI\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "mykey=os.getenv(\"OpenAI_Key\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Windows\\Temp\\ipykernel_31712\\429612894.py:1: LangChainDeprecationWarning: The class `ChatOpenAI` was deprecated in LangChain 0.0.10 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-openai package and should be used instead. To use it run `pip install -U :class:`~langchain-openai` and import as `from :class:`~langchain_openai import ChatOpenAI``.\n",
      "  llm = ChatOpenAI(openai_api_key=mykey, model=\"gpt-3.5-turbo\", temperature=0)\n"
     ]
    }
   ],
   "source": [
    "llm = ChatOpenAI(openai_api_key=mykey, model=\"gpt-3.5-turbo\", temperature=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['LangChainDeprecationWarning', '__all__', '__builtins__', '__cached__', '__doc__', '__file__', '__getattr__', '__loader__', '__name__', '__package__', '__path__', '__spec__', 'base', 'init_chat_model', 'is_interactive_env', 'warnings']\n"
     ]
    }
   ],
   "source": [
    "import langchain.chat_models\n",
    "print(dir(langchain.chat_models))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "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\n"
     ]
    }
   ],
   "source": [
    "print(mykey)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.llms import OpenAI\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.chains import LLMChain\n",
    "from langchain.chains import SequentialChain\n",
    "from langchain.callbacks import get_openai_callback\n",
    "import PyPDF2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "RESPONSE_JSON = {\n",
    "    \"1\": {\n",
    "        \"mcq\": \"multiple choice question\",\n",
    "        \"options\": {\n",
    "            \"a\": \"choice here\",\n",
    "            \"b\": \"choice here\",\n",
    "            \"c\": \"choice here\",\n",
    "            \"d\": \"choice here\",\n",
    "        },\n",
    "        \"correct\": \"correct answer\",\n",
    "    },\n",
    "    \"2\": {\n",
    "        \"mcq\": \"multiple choice question\",\n",
    "        \"options\": {\n",
    "            \"a\": \"choice here\",\n",
    "            \"b\": \"choice here\",\n",
    "            \"c\": \"choice here\",\n",
    "            \"d\": \"choice here\",\n",
    "        },\n",
    "        \"correct\": \"correct answer\",\n",
    "    },\n",
    "    \"3\": {\n",
    "        \"mcq\": \"multiple choice question\",\n",
    "        \"options\": {\n",
    "            \"a\": \"choice here\",\n",
    "            \"b\": \"choice here\",\n",
    "            \"c\": \"choice here\",\n",
    "            \"d\": \"choice here\",\n",
    "        },\n",
    "        \"correct\": \"correct answer\",\n",
    "    },\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "TEMPLATE=\"\"\"\n",
    "Text:{text}\n",
    "You are an expert MCQ maker. Given the above text, it is your job to \\\n",
    "create a quiz  of {number} multiple choice questions for {subject} students in {tone} tone. \n",
    "Make sure the questions are not repeated and check all the questions to be conforming the text as well.\n",
    "Make sure to format your response like  RESPONSE_JSON below  and use it as a guide. \\\n",
    "Ensure to make {number} MCQs\n",
    "### RESPONSE_JSON\n",
    "{response_json}\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "quiz_generation_prompt = PromptTemplate(\n",
    "    input_variables=[\"text\", \"number\", \"subject\", \"tone\", \"response_json\"],\n",
    "    template=TEMPLATE\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Windows\\Temp\\ipykernel_31712\\3797420763.py:1: LangChainDeprecationWarning: The class `LLMChain` was deprecated in LangChain 0.1.17 and will be removed in 1.0. Use :meth:`~RunnableSequence, e.g., `prompt | llm`` instead.\n",
      "  quiz_chain=LLMChain(llm=llm, prompt=quiz_generation_prompt, output_key=\"quiz\", verbose=True)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "quiz_chain=LLMChain(llm=llm, prompt=quiz_generation_prompt, output_key=\"quiz\", verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "TEMPLATE2=\"\"\"\n",
    "You are an expert english grammarian and writer. Given a Multiple Choice Quiz for {subject} students.\\\n",
    "You need to evaluate the complexity of the question and give a complete analysis of the quiz. Only use at max 50 words for complexity analysis. \n",
    "if the quiz is not at per with the cognitive and analytical abilities of the students,\\\n",
    "update the quiz questions which needs to be changed and change the tone such that it perfectly fits the student abilities\n",
    "Quiz_MCQs:\n",
    "{quiz}\n",
    "\n",
    "Check from an expert English Writer of the above quiz:\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "quiz_evaluation_prompt=PromptTemplate(input_variables=[\"subject\", \"quiz\"], template=TEMPLATE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "review_chain=LLMChain(llm=llm, prompt=quiz_evaluation_prompt, output_key=\"review\", verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "generate_evaluate_chain=SequentialChain(chains=[quiz_chain, review_chain], input_variables=[\"text\", \"number\", \"subject\", \"tone\", \"response_json\"],\n",
    "                                        output_variables=[\"quiz\", \"review\"], verbose=True,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path=r\"C:\\Users\\acer\\AdamGen\\data.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'C:\\\\Users\\\\acer\\\\AdamGen\\\\data.txt'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "file_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(file_path, 'r') as file:\n",
    "    TEXT = file.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Polynomial Regression\n",
      "2.3.1 Polynomial Regression Intuition\n",
      "Is Polynomial Regression a linear or non linear model?\n",
      "That depends on what you are referring to. Polynomial Regression is linear on the coefficients since we don’t\n",
      "have any power of the coefficients (all the coefficients are raised to the power of 1: b0, b1, ..., bn). However,\n",
      "Polynomial Regression is a non linear function of the input x, since we have the inputs raised to several\n",
      "powers: x (power 1), x\n",
      "2\n",
      "(power 2), ..., x\n",
      "n (power n). That is how we can also see the Polynomial Regression\n",
      "as a non linear model. Besides indeed, Polynomial Regression is appropriate when the data is non linearly\n",
      "distributed (meaning you can’t fit a straight line between y and x).\n",
      "Page 8 of 51\n",
      "Machine Learning A-Z Q&A\n",
      "2.3.2 Polynomial Regression in Python\n",
      "Why didn’t we apply Feature Scaling in our Polynomial Regression model?\n",
      "It’s simply because, since y is a linear combination of x and x\n",
      "2\n",
      ", the coefficients can adapt their scale to put\n",
      "everything on the same scale. For example if y takes values between 0 and 1, x takes values between 1 and\n",
      "10 and x\n",
      "2\n",
      "takes values between 1 and 100, then b1 can be multiplied by 0.1 and b2 can be multiplied by 0.01\n",
      "so that y, b1x1 and b2x2 are all on the same scale.\n",
      "How do we find the best degree?\n",
      "The main form of finding a good fit is to plot the model and see what it looks like visually. You simply\n",
      "test several degrees and you see which one gives you the best fit. The other option is to find the lowest\n",
      "root-mean-square error (RMSE) for your model, but in that case be careful not to overfit the data.\n",
      "Why did we have to create a second linear regressor ’lin_reg_2’ ? Could we not have used\n",
      "’lin_reg’ directly?\n",
      "No, because ’lin_reg’ is already fitted to X and y and now we want to fit a new linear model to Xpoly and\n",
      "y. So we have to create a new regressor object. One must important that the fit method here finds the\n",
      "coefficient between the independent variables and the dependent variable. Therefore since ’lin_reg’ already\n",
      "got the coefficients of correlation between X and y, ’lin_reg_2’ has to be created to get some new coefficients\n",
      "of correlations between Xpoly and y.\n",
      "2.3.3 Polynomial Regression in R\n",
      "In R, I have to create manually the different columns for each polynomial feature? What if\n",
      "there are many polynomial features?\n",
      "You will very rarely have to create more than 4 polynomial features, otherwise you would get overfitting,\n",
      "which must absolutely be avoided in Machine Learning. So even if you have to create them manually, it will\n",
      "never take too much of your time. Besides you can use the template.\n",
      "How do we find the best degree? (Asking this question again in case some students only do R\n",
      "The main form of finding a good fit is to plot the model and see what it looks like visually. You simply\n",
      "test several degrees and you see which one gives you the best fit. The other option is to find the lowest\n",
      "root-mean-square error (RMSE) for your model, but in that case be careful not to overfit the data.\n",
      "2.4 SVR\n",
      "2.4.1 SVR Intuition\n",
      "When should I use SVR?\n",
      "You should use SVR if a linear model like linear regression doesn’t fit very well your data. This would mean\n",
      "you are dealing with a non linear problem, where your data is not linearly distributed. Therefore in that\n",
      "case SVR could be a much better solution.\n",
      "I didn’t understand the Intuition Lecture. Am I in trouble?\n",
      "Not at all. SVR is a pretty abstract model and besides it is not that commonly used. What you must rather\n",
      "understand is the SVM model, which you will see in Part 3 - Classification. Then once you understand the\n",
      "SVM model, you will get a better grasp of the SVR model, since the SVR is simply the SVM for Regression.\n",
      "However we wanted to include SVR in this course to give you an extra option in your Machine Learning\n",
      "toolkit.\n",
      "Page 9 of 51\n",
      "Machine Learning A-Z Q&A\n",
      "2.4.2 SVR in Python\n",
      "Why do we need to ’sc_Y.inverse_transform’ ?\n",
      "We need the inverse_transform method to go back to the original scale. Indeed we applied feature scaling\n",
      "so we get this scale around 0 and if we make a prediction without inversing the scale we will get the\n",
      "scaled predicted salary. And of course we want the real salary, not the scaled one, so we have to use\n",
      "’sc_Y.inverse_transform’. Also what is important to understand is that ’transform’ and ’inverse_transform’\n",
      "are paired methods.\n",
      "2.4.3 SVR in R\n",
      "Why did we not apply feature scaling like we did explicitly in Python\n",
      "That’s because in svm() function of R, the values are automatically scaled.\n",
      "Can we select the most significant variables thanks to the p-value like we did in R before?\n",
      "You couldn’t use p-value because SVR is not a linear model, and p-values apply only to linear models.\n",
      "Therefore feature selection is out of the question. But you could do feature extraction, which you will see in\n",
      "Part 9 - Dimensionality Reduction. That you can apply to Decision Trees, and it will reduce the number of\n",
      "your features.\n",
      "2.5 Decision Tree Regression\n",
      "2.5.1 Decision Tree Regression Intuition\n",
      "How does the algorithm split the data points?\n",
      "It uses reduction of standard deviation of the predictions. In other words, the standard deviation is decreased\n",
      "right after a split. Hence, building a decision tree is all about finding the attribute that returns the highest\n",
      "standard deviation reduction (i.e., the most homogeneous branches).\n",
      "What is the Information Gain and how does it work in Decision Trees?\n",
      "The Information Gain in Decision Tree Regression is exactly the Standard Deviation Reduction we are\n",
      "looking to reach. We calculate by how much the Standard Deviation decreases after each split. Because the\n",
      "more the Standard Deviation is decreased after a split, the more homogeneous the child nodes will be.\n",
      "What is the Entropy and how does it work in Decision Trees?\n",
      "The Entropy measures the disorder in a set, here in a part resulting from a split. So the more homogeneous\n",
      "is your data in a part, the lower will be the entropy. The more you have splits, the more you have chance\n",
      "to find parts in which your data is homogeneous, and therefore the lower will be the entropy (close to 0) in\n",
      "these parts. However you might still find some nodes where the data is not homogeneous, and therefore the\n",
      "entropy would not be that small.\n",
      "2.5.2 Decision Tree Regression in Python\n",
      "Does a Decision Tree make much sense in 1D?\n",
      "Not really, as we saw in the practical part of this section. In 1D (meaning one independent variable), the\n",
      "Decision Tree clearly tends to overfit the data. The Decision Tree would be much more relevant in higher\n",
      "dimension, but keep in mind that the implementation we made here in 1D would be exactly the same in\n",
      "higher dimension. Therefore you might want to keep that model in your toolkit in case you are dealing\n",
      "with a higher dimensional space. This will actually be the case in Part 3 - Classification, where we will use\n",
      "Decision Tree for Classification in 2D, which you will see turns out to be more relevant.\n",
      "Page 10 of 51\n",
      "Machine Learning A-Z Q&A\n",
      "2.5.3 Decision Tree Regression in R\n",
      "Why do we get different results between Python and R?\n",
      "The difference is likely due to the random split of data. If we did a cross-validation (see Part 10) on all the\n",
      "models in both languages, then you would likely get a similar mean accuracy. That being said, we would\n",
      "recommend more using Python for Decision Trees since the model is slightly better implemented in Python.\n",
      "Is the Decision Tree appropriate here?\n",
      "Here in this example, we can clearly see that the fitting curve is a stair with large gaps in the discontinuities.\n",
      "That decision tree regression model is therefore not the most appropriate, and that is because we have only\n",
      "one independent variables taking discrete values. So what happened is that the prediction was made in the\n",
      "lower part of the gap in Python, and made in the upper part of the gap in R. And since the gap is large, that\n",
      "makes a big difference. If we had much more observations, taking values with more continuity (like with a\n",
      "0.1 step), the gaps would be smaller and therefore the predictions in Python and R far from each other.\n",
      "Can we select the most significant variables thanks to the p-value like we did in R before?\n",
      "You couldn’t use p-value because Decision Tree is not a linear model, and p-values apply only to linear\n",
      "models. Therefore feature selection is out of the question. But you could do feature extraction, which you\n",
      "will see in Part 9 - Dimensionality Reduction. That you can apply to Decision Trees, and it will reduce the\n",
      "number of your features.\n",
      "2.6 Random Forest Regression\n",
      "2.6.1 Random Forest Regression Intuition\n",
      "What is the advantage and drawback of Random Forests compared to Decision Trees? Advan\u0002tage: Random Forests can give you a better predictive power than Decision Trees.\n",
      "Drawback: Decision Tree will give you more interpretability than Random Forests, because you can plot the\n",
      "graph of a Decision Tree to see the different splits leading to the prediction, as seen in the Intuition Lecture.\n",
      "That’s something you can’t do with Random Forests.\n",
      "When to use Random Forest and when to use the other models?\n",
      "The best answer to that question is: try them all!\n",
      "Indeed, thanks to the templates it will only take you 10 minutes to try all the models, which is very little\n",
      "compared to the time dedicated to the other parts of a data science project (like Data Preprocessing for\n",
      "example). So just don’t be scared to try all the regression models and compare the results (through cross\n",
      "validation which we will see in Part \n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(TEXT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'{\"1\": {\"mcq\": \"multiple choice question\", \"options\": {\"a\": \"choice here\", \"b\": \"choice here\", \"c\": \"choice here\", \"d\": \"choice here\"}, \"correct\": \"correct answer\"}, \"2\": {\"mcq\": \"multiple choice question\", \"options\": {\"a\": \"choice here\", \"b\": \"choice here\", \"c\": \"choice here\", \"d\": \"choice here\"}, \"correct\": \"correct answer\"}, \"3\": {\"mcq\": \"multiple choice question\", \"options\": {\"a\": \"choice here\", \"b\": \"choice here\", \"c\": \"choice here\", \"d\": \"choice here\"}, \"correct\": \"correct answer\"}}'"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Serialize the Python dictionary into a JSON-formatted string\n",
    "json.dumps(RESPONSE_JSON)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUMBER=8\n",
    "SUBJECT=\"Machine Learning\"\n",
    "TONE=\"simple\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new SequentialChain chain...\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "Text:Polynomial Regression\n",
      "2.3.1 Polynomial Regression Intuition\n",
      "Is Polynomial Regression a linear or non linear model?\n",
      "That depends on what you are referring to. Polynomial Regression is linear on the coefficients since we don’t\n",
      "have any power of the coefficients (all the coefficients are raised to the power of 1: b0, b1, ..., bn). However,\n",
      "Polynomial Regression is a non linear function of the input x, since we have the inputs raised to several\n",
      "powers: x (power 1), x\n",
      "2\n",
      "(power 2), ..., x\n",
      "n (power n). That is how we can also see the Polynomial Regression\n",
      "as a non linear model. Besides indeed, Polynomial Regression is appropriate when the data is non linearly\n",
      "distributed (meaning you can’t fit a straight line between y and x).\n",
      "Page 8 of 51\n",
      "Machine Learning A-Z Q&A\n",
      "2.3.2 Polynomial Regression in Python\n",
      "Why didn’t we apply Feature Scaling in our Polynomial Regression model?\n",
      "It’s simply because, since y is a linear combination of x and x\n",
      "2\n",
      ", the coefficients can adapt their scale to put\n",
      "everything on the same scale. For example if y takes values between 0 and 1, x takes values between 1 and\n",
      "10 and x\n",
      "2\n",
      "takes values between 1 and 100, then b1 can be multiplied by 0.1 and b2 can be multiplied by 0.01\n",
      "so that y, b1x1 and b2x2 are all on the same scale.\n",
      "How do we find the best degree?\n",
      "The main form of finding a good fit is to plot the model and see what it looks like visually. You simply\n",
      "test several degrees and you see which one gives you the best fit. The other option is to find the lowest\n",
      "root-mean-square error (RMSE) for your model, but in that case be careful not to overfit the data.\n",
      "Why did we have to create a second linear regressor ’lin_reg_2’ ? Could we not have used\n",
      "’lin_reg’ directly?\n",
      "No, because ’lin_reg’ is already fitted to X and y and now we want to fit a new linear model to Xpoly and\n",
      "y. So we have to create a new regressor object. One must important that the fit method here finds the\n",
      "coefficient between the independent variables and the dependent variable. Therefore since ’lin_reg’ already\n",
      "got the coefficients of correlation between X and y, ’lin_reg_2’ has to be created to get some new coefficients\n",
      "of correlations between Xpoly and y.\n",
      "2.3.3 Polynomial Regression in R\n",
      "In R, I have to create manually the different columns for each polynomial feature? What if\n",
      "there are many polynomial features?\n",
      "You will very rarely have to create more than 4 polynomial features, otherwise you would get overfitting,\n",
      "which must absolutely be avoided in Machine Learning. So even if you have to create them manually, it will\n",
      "never take too much of your time. Besides you can use the template.\n",
      "How do we find the best degree? (Asking this question again in case some students only do R\n",
      "The main form of finding a good fit is to plot the model and see what it looks like visually. You simply\n",
      "test several degrees and you see which one gives you the best fit. The other option is to find the lowest\n",
      "root-mean-square error (RMSE) for your model, but in that case be careful not to overfit the data.\n",
      "2.4 SVR\n",
      "2.4.1 SVR Intuition\n",
      "When should I use SVR?\n",
      "You should use SVR if a linear model like linear regression doesn’t fit very well your data. This would mean\n",
      "you are dealing with a non linear problem, where your data is not linearly distributed. Therefore in that\n",
      "case SVR could be a much better solution.\n",
      "I didn’t understand the Intuition Lecture. Am I in trouble?\n",
      "Not at all. SVR is a pretty abstract model and besides it is not that commonly used. What you must rather\n",
      "understand is the SVM model, which you will see in Part 3 - Classification. Then once you understand the\n",
      "SVM model, you will get a better grasp of the SVR model, since the SVR is simply the SVM for Regression.\n",
      "However we wanted to include SVR in this course to give you an extra option in your Machine Learning\n",
      "toolkit.\n",
      "Page 9 of 51\n",
      "Machine Learning A-Z Q&A\n",
      "2.4.2 SVR in Python\n",
      "Why do we need to ’sc_Y.inverse_transform’ ?\n",
      "We need the inverse_transform method to go back to the original scale. Indeed we applied feature scaling\n",
      "so we get this scale around 0 and if we make a prediction without inversing the scale we will get the\n",
      "scaled predicted salary. And of course we want the real salary, not the scaled one, so we have to use\n",
      "’sc_Y.inverse_transform’. Also what is important to understand is that ’transform’ and ’inverse_transform’\n",
      "are paired methods.\n",
      "2.4.3 SVR in R\n",
      "Why did we not apply feature scaling like we did explicitly in Python\n",
      "That’s because in svm() function of R, the values are automatically scaled.\n",
      "Can we select the most significant variables thanks to the p-value like we did in R before?\n",
      "You couldn’t use p-value because SVR is not a linear model, and p-values apply only to linear models.\n",
      "Therefore feature selection is out of the question. But you could do feature extraction, which you will see in\n",
      "Part 9 - Dimensionality Reduction. That you can apply to Decision Trees, and it will reduce the number of\n",
      "your features.\n",
      "2.5 Decision Tree Regression\n",
      "2.5.1 Decision Tree Regression Intuition\n",
      "How does the algorithm split the data points?\n",
      "It uses reduction of standard deviation of the predictions. In other words, the standard deviation is decreased\n",
      "right after a split. Hence, building a decision tree is all about finding the attribute that returns the highest\n",
      "standard deviation reduction (i.e., the most homogeneous branches).\n",
      "What is the Information Gain and how does it work in Decision Trees?\n",
      "The Information Gain in Decision Tree Regression is exactly the Standard Deviation Reduction we are\n",
      "looking to reach. We calculate by how much the Standard Deviation decreases after each split. Because the\n",
      "more the Standard Deviation is decreased after a split, the more homogeneous the child nodes will be.\n",
      "What is the Entropy and how does it work in Decision Trees?\n",
      "The Entropy measures the disorder in a set, here in a part resulting from a split. So the more homogeneous\n",
      "is your data in a part, the lower will be the entropy. The more you have splits, the more you have chance\n",
      "to find parts in which your data is homogeneous, and therefore the lower will be the entropy (close to 0) in\n",
      "these parts. However you might still find some nodes where the data is not homogeneous, and therefore the\n",
      "entropy would not be that small.\n",
      "2.5.2 Decision Tree Regression in Python\n",
      "Does a Decision Tree make much sense in 1D?\n",
      "Not really, as we saw in the practical part of this section. In 1D (meaning one independent variable), the\n",
      "Decision Tree clearly tends to overfit the data. The Decision Tree would be much more relevant in higher\n",
      "dimension, but keep in mind that the implementation we made here in 1D would be exactly the same in\n",
      "higher dimension. Therefore you might want to keep that model in your toolkit in case you are dealing\n",
      "with a higher dimensional space. This will actually be the case in Part 3 - Classification, where we will use\n",
      "Decision Tree for Classification in 2D, which you will see turns out to be more relevant.\n",
      "Page 10 of 51\n",
      "Machine Learning A-Z Q&A\n",
      "2.5.3 Decision Tree Regression in R\n",
      "Why do we get different results between Python and R?\n",
      "The difference is likely due to the random split of data. If we did a cross-validation (see Part 10) on all the\n",
      "models in both languages, then you would likely get a similar mean accuracy. That being said, we would\n",
      "recommend more using Python for Decision Trees since the model is slightly better implemented in Python.\n",
      "Is the Decision Tree appropriate here?\n",
      "Here in this example, we can clearly see that the fitting curve is a stair with large gaps in the discontinuities.\n",
      "That decision tree regression model is therefore not the most appropriate, and that is because we have only\n",
      "one independent variables taking discrete values. So what happened is that the prediction was made in the\n",
      "lower part of the gap in Python, and made in the upper part of the gap in R. And since the gap is large, that\n",
      "makes a big difference. If we had much more observations, taking values with more continuity (like with a\n",
      "0.1 step), the gaps would be smaller and therefore the predictions in Python and R far from each other.\n",
      "Can we select the most significant variables thanks to the p-value like we did in R before?\n",
      "You couldn’t use p-value because Decision Tree is not a linear model, and p-values apply only to linear\n",
      "models. Therefore feature selection is out of the question. But you could do feature extraction, which you\n",
      "will see in Part 9 - Dimensionality Reduction. That you can apply to Decision Trees, and it will reduce the\n",
      "number of your features.\n",
      "2.6 Random Forest Regression\n",
      "2.6.1 Random Forest Regression Intuition\n",
      "What is the advantage and drawback of Random Forests compared to Decision Trees? Advan\u0002tage: Random Forests can give you a better predictive power than Decision Trees.\n",
      "Drawback: Decision Tree will give you more interpretability than Random Forests, because you can plot the\n",
      "graph of a Decision Tree to see the different splits leading to the prediction, as seen in the Intuition Lecture.\n",
      "That’s something you can’t do with Random Forests.\n",
      "When to use Random Forest and when to use the other models?\n",
      "The best answer to that question is: try them all!\n",
      "Indeed, thanks to the templates it will only take you 10 minutes to try all the models, which is very little\n",
      "compared to the time dedicated to the other parts of a data science project (like Data Preprocessing for\n",
      "example). So just don’t be scared to try all the regression models and compare the results (through cross\n",
      "validation which we will see in Part \n",
      "\n",
      "You are an expert MCQ maker. Given the above text, it is your job to create a quiz  of 8 multiple choice questions for Machine Learning students in simple tone. \n",
      "Make sure the questions are not repeated and check all the questions to be conforming the text as well.\n",
      "Make sure to format your response like  RESPONSE_JSON below  and use it as a guide. Ensure to make 8 MCQs\n",
      "### RESPONSE_JSON\n",
      "{\"1\": {\"mcq\": \"multiple choice question\", \"options\": {\"a\": \"choice here\", \"b\": \"choice here\", \"c\": \"choice here\", \"d\": \"choice here\"}, \"correct\": \"correct answer\"}, \"2\": {\"mcq\": \"multiple choice question\", \"options\": {\"a\": \"choice here\", \"b\": \"choice here\", \"c\": \"choice here\", \"d\": \"choice here\"}, \"correct\": \"correct answer\"}, \"3\": {\"mcq\": \"multiple choice question\", \"options\": {\"a\": \"choice here\", \"b\": \"choice here\", \"c\": \"choice here\", \"d\": \"choice here\"}, \"correct\": \"correct answer\"}}\n",
      "\n",
      "\u001b[0m\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Windows\\Temp\\ipykernel_31712\\2654919708.py:5: LangChainDeprecationWarning: The method `Chain.__call__` was deprecated in langchain 0.1.0 and will be removed in 1.0. Use :meth:`~invoke` instead.\n",
      "  response=generate_evaluate_chain(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "Text:Polynomial Regression\n",
      "2.3.1 Polynomial Regression Intuition\n",
      "Is Polynomial Regression a linear or non linear model?\n",
      "That depends on what you are referring to. Polynomial Regression is linear on the coefficients since we don’t\n",
      "have any power of the coefficients (all the coefficients are raised to the power of 1: b0, b1, ..., bn). However,\n",
      "Polynomial Regression is a non linear function of the input x, since we have the inputs raised to several\n",
      "powers: x (power 1), x\n",
      "2\n",
      "(power 2), ..., x\n",
      "n (power n). That is how we can also see the Polynomial Regression\n",
      "as a non linear model. Besides indeed, Polynomial Regression is appropriate when the data is non linearly\n",
      "distributed (meaning you can’t fit a straight line between y and x).\n",
      "Page 8 of 51\n",
      "Machine Learning A-Z Q&A\n",
      "2.3.2 Polynomial Regression in Python\n",
      "Why didn’t we apply Feature Scaling in our Polynomial Regression model?\n",
      "It’s simply because, since y is a linear combination of x and x\n",
      "2\n",
      ", the coefficients can adapt their scale to put\n",
      "everything on the same scale. For example if y takes values between 0 and 1, x takes values between 1 and\n",
      "10 and x\n",
      "2\n",
      "takes values between 1 and 100, then b1 can be multiplied by 0.1 and b2 can be multiplied by 0.01\n",
      "so that y, b1x1 and b2x2 are all on the same scale.\n",
      "How do we find the best degree?\n",
      "The main form of finding a good fit is to plot the model and see what it looks like visually. You simply\n",
      "test several degrees and you see which one gives you the best fit. The other option is to find the lowest\n",
      "root-mean-square error (RMSE) for your model, but in that case be careful not to overfit the data.\n",
      "Why did we have to create a second linear regressor ’lin_reg_2’ ? Could we not have used\n",
      "’lin_reg’ directly?\n",
      "No, because ’lin_reg’ is already fitted to X and y and now we want to fit a new linear model to Xpoly and\n",
      "y. So we have to create a new regressor object. One must important that the fit method here finds the\n",
      "coefficient between the independent variables and the dependent variable. Therefore since ’lin_reg’ already\n",
      "got the coefficients of correlation between X and y, ’lin_reg_2’ has to be created to get some new coefficients\n",
      "of correlations between Xpoly and y.\n",
      "2.3.3 Polynomial Regression in R\n",
      "In R, I have to create manually the different columns for each polynomial feature? What if\n",
      "there are many polynomial features?\n",
      "You will very rarely have to create more than 4 polynomial features, otherwise you would get overfitting,\n",
      "which must absolutely be avoided in Machine Learning. So even if you have to create them manually, it will\n",
      "never take too much of your time. Besides you can use the template.\n",
      "How do we find the best degree? (Asking this question again in case some students only do R\n",
      "The main form of finding a good fit is to plot the model and see what it looks like visually. You simply\n",
      "test several degrees and you see which one gives you the best fit. The other option is to find the lowest\n",
      "root-mean-square error (RMSE) for your model, but in that case be careful not to overfit the data.\n",
      "2.4 SVR\n",
      "2.4.1 SVR Intuition\n",
      "When should I use SVR?\n",
      "You should use SVR if a linear model like linear regression doesn’t fit very well your data. This would mean\n",
      "you are dealing with a non linear problem, where your data is not linearly distributed. Therefore in that\n",
      "case SVR could be a much better solution.\n",
      "I didn’t understand the Intuition Lecture. Am I in trouble?\n",
      "Not at all. SVR is a pretty abstract model and besides it is not that commonly used. What you must rather\n",
      "understand is the SVM model, which you will see in Part 3 - Classification. Then once you understand the\n",
      "SVM model, you will get a better grasp of the SVR model, since the SVR is simply the SVM for Regression.\n",
      "However we wanted to include SVR in this course to give you an extra option in your Machine Learning\n",
      "toolkit.\n",
      "Page 9 of 51\n",
      "Machine Learning A-Z Q&A\n",
      "2.4.2 SVR in Python\n",
      "Why do we need to ’sc_Y.inverse_transform’ ?\n",
      "We need the inverse_transform method to go back to the original scale. Indeed we applied feature scaling\n",
      "so we get this scale around 0 and if we make a prediction without inversing the scale we will get the\n",
      "scaled predicted salary. And of course we want the real salary, not the scaled one, so we have to use\n",
      "’sc_Y.inverse_transform’. Also what is important to understand is that ’transform’ and ’inverse_transform’\n",
      "are paired methods.\n",
      "2.4.3 SVR in R\n",
      "Why did we not apply feature scaling like we did explicitly in Python\n",
      "That’s because in svm() function of R, the values are automatically scaled.\n",
      "Can we select the most significant variables thanks to the p-value like we did in R before?\n",
      "You couldn’t use p-value because SVR is not a linear model, and p-values apply only to linear models.\n",
      "Therefore feature selection is out of the question. But you could do feature extraction, which you will see in\n",
      "Part 9 - Dimensionality Reduction. That you can apply to Decision Trees, and it will reduce the number of\n",
      "your features.\n",
      "2.5 Decision Tree Regression\n",
      "2.5.1 Decision Tree Regression Intuition\n",
      "How does the algorithm split the data points?\n",
      "It uses reduction of standard deviation of the predictions. In other words, the standard deviation is decreased\n",
      "right after a split. Hence, building a decision tree is all about finding the attribute that returns the highest\n",
      "standard deviation reduction (i.e., the most homogeneous branches).\n",
      "What is the Information Gain and how does it work in Decision Trees?\n",
      "The Information Gain in Decision Tree Regression is exactly the Standard Deviation Reduction we are\n",
      "looking to reach. We calculate by how much the Standard Deviation decreases after each split. Because the\n",
      "more the Standard Deviation is decreased after a split, the more homogeneous the child nodes will be.\n",
      "What is the Entropy and how does it work in Decision Trees?\n",
      "The Entropy measures the disorder in a set, here in a part resulting from a split. So the more homogeneous\n",
      "is your data in a part, the lower will be the entropy. The more you have splits, the more you have chance\n",
      "to find parts in which your data is homogeneous, and therefore the lower will be the entropy (close to 0) in\n",
      "these parts. However you might still find some nodes where the data is not homogeneous, and therefore the\n",
      "entropy would not be that small.\n",
      "2.5.2 Decision Tree Regression in Python\n",
      "Does a Decision Tree make much sense in 1D?\n",
      "Not really, as we saw in the practical part of this section. In 1D (meaning one independent variable), the\n",
      "Decision Tree clearly tends to overfit the data. The Decision Tree would be much more relevant in higher\n",
      "dimension, but keep in mind that the implementation we made here in 1D would be exactly the same in\n",
      "higher dimension. Therefore you might want to keep that model in your toolkit in case you are dealing\n",
      "with a higher dimensional space. This will actually be the case in Part 3 - Classification, where we will use\n",
      "Decision Tree for Classification in 2D, which you will see turns out to be more relevant.\n",
      "Page 10 of 51\n",
      "Machine Learning A-Z Q&A\n",
      "2.5.3 Decision Tree Regression in R\n",
      "Why do we get different results between Python and R?\n",
      "The difference is likely due to the random split of data. If we did a cross-validation (see Part 10) on all the\n",
      "models in both languages, then you would likely get a similar mean accuracy. That being said, we would\n",
      "recommend more using Python for Decision Trees since the model is slightly better implemented in Python.\n",
      "Is the Decision Tree appropriate here?\n",
      "Here in this example, we can clearly see that the fitting curve is a stair with large gaps in the discontinuities.\n",
      "That decision tree regression model is therefore not the most appropriate, and that is because we have only\n",
      "one independent variables taking discrete values. So what happened is that the prediction was made in the\n",
      "lower part of the gap in Python, and made in the upper part of the gap in R. And since the gap is large, that\n",
      "makes a big difference. If we had much more observations, taking values with more continuity (like with a\n",
      "0.1 step), the gaps would be smaller and therefore the predictions in Python and R far from each other.\n",
      "Can we select the most significant variables thanks to the p-value like we did in R before?\n",
      "You couldn’t use p-value because Decision Tree is not a linear model, and p-values apply only to linear\n",
      "models. Therefore feature selection is out of the question. But you could do feature extraction, which you\n",
      "will see in Part 9 - Dimensionality Reduction. That you can apply to Decision Trees, and it will reduce the\n",
      "number of your features.\n",
      "2.6 Random Forest Regression\n",
      "2.6.1 Random Forest Regression Intuition\n",
      "What is the advantage and drawback of Random Forests compared to Decision Trees? Advan\u0002tage: Random Forests can give you a better predictive power than Decision Trees.\n",
      "Drawback: Decision Tree will give you more interpretability than Random Forests, because you can plot the\n",
      "graph of a Decision Tree to see the different splits leading to the prediction, as seen in the Intuition Lecture.\n",
      "That’s something you can’t do with Random Forests.\n",
      "When to use Random Forest and when to use the other models?\n",
      "The best answer to that question is: try them all!\n",
      "Indeed, thanks to the templates it will only take you 10 minutes to try all the models, which is very little\n",
      "compared to the time dedicated to the other parts of a data science project (like Data Preprocessing for\n",
      "example). So just don’t be scared to try all the regression models and compare the results (through cross\n",
      "validation which we will see in Part \n",
      "\n",
      "You are an expert MCQ maker. Given the above text, it is your job to create a quiz  of 8 multiple choice questions for Machine Learning students in simple tone. \n",
      "Make sure the questions are not repeated and check all the questions to be conforming the text as well.\n",
      "Make sure to format your response like  RESPONSE_JSON below  and use it as a guide. Ensure to make 8 MCQs\n",
      "### RESPONSE_JSON\n",
      "{\"1\": {\"mcq\": \"multiple choice question\", \"options\": {\"a\": \"choice here\", \"b\": \"choice here\", \"c\": \"choice here\", \"d\": \"choice here\"}, \"correct\": \"correct answer\"}, \"2\": {\"mcq\": \"multiple choice question\", \"options\": {\"a\": \"choice here\", \"b\": \"choice here\", \"c\": \"choice here\", \"d\": \"choice here\"}, \"correct\": \"correct answer\"}, \"3\": {\"mcq\": \"multiple choice question\", \"options\": {\"a\": \"choice here\", \"b\": \"choice here\", \"c\": \"choice here\", \"d\": \"choice here\"}, \"correct\": \"correct answer\"}}\n",
      "\n",
      "\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "#https://python.langchain.com/docs/modules/model_io/llms/token_usage_tracking\n",
    "\n",
    "#How to setup Token Usage Tracking in LangChain\n",
    "with get_openai_callback() as cb:\n",
    "    response=generate_evaluate_chain(\n",
    "        {\n",
    "            \"text\": TEXT,\n",
    "            \"number\": NUMBER,\n",
    "            \"subject\":SUBJECT,\n",
    "            \"tone\": TONE,\n",
    "            \"response_json\": json.dumps(RESPONSE_JSON)\n",
    "        }\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Tokens:6192\n",
      "Prompt Tokens:4820\n",
      "Completion Tokens:1372\n",
      "Total Cost:0.009974\n"
     ]
    }
   ],
   "source": [
    "print(f\"Total Tokens:{cb.total_tokens}\")\n",
    "print(f\"Prompt Tokens:{cb.prompt_tokens}\")\n",
    "print(f\"Completion Tokens:{cb.completion_tokens}\")\n",
    "print(f\"Total Cost:{cb.total_cost}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'text': 'Polynomial Regression\\n2.3.1 Polynomial Regression Intuition\\nIs Polynomial Regression a linear or non linear model?\\nThat depends on what you are referring to. Polynomial Regression is linear on the coefficients since we don’t\\nhave any power of the coefficients (all the coefficients are raised to the power of 1: b0, b1, ..., bn). However,\\nPolynomial Regression is a non linear function of the input x, since we have the inputs raised to several\\npowers: x (power 1), x\\n2\\n(power 2), ..., x\\nn (power n). That is how we can also see the Polynomial Regression\\nas a non linear model. Besides indeed, Polynomial Regression is appropriate when the data is non linearly\\ndistributed (meaning you can’t fit a straight line between y and x).\\nPage 8 of 51\\nMachine Learning A-Z Q&A\\n2.3.2 Polynomial Regression in Python\\nWhy didn’t we apply Feature Scaling in our Polynomial Regression model?\\nIt’s simply because, since y is a linear combination of x and x\\n2\\n, the coefficients can adapt their scale to put\\neverything on the same scale. For example if y takes values between 0 and 1, x takes values between 1 and\\n10 and x\\n2\\ntakes values between 1 and 100, then b1 can be multiplied by 0.1 and b2 can be multiplied by 0.01\\nso that y, b1x1 and b2x2 are all on the same scale.\\nHow do we find the best degree?\\nThe main form of finding a good fit is to plot the model and see what it looks like visually. You simply\\ntest several degrees and you see which one gives you the best fit. The other option is to find the lowest\\nroot-mean-square error (RMSE) for your model, but in that case be careful not to overfit the data.\\nWhy did we have to create a second linear regressor ’lin_reg_2’ ? Could we not have used\\n’lin_reg’ directly?\\nNo, because ’lin_reg’ is already fitted to X and y and now we want to fit a new linear model to Xpoly and\\ny. So we have to create a new regressor object. One must important that the fit method here finds the\\ncoefficient between the independent variables and the dependent variable. Therefore since ’lin_reg’ already\\ngot the coefficients of correlation between X and y, ’lin_reg_2’ has to be created to get some new coefficients\\nof correlations between Xpoly and y.\\n2.3.3 Polynomial Regression in R\\nIn R, I have to create manually the different columns for each polynomial feature? What if\\nthere are many polynomial features?\\nYou will very rarely have to create more than 4 polynomial features, otherwise you would get overfitting,\\nwhich must absolutely be avoided in Machine Learning. So even if you have to create them manually, it will\\nnever take too much of your time. Besides you can use the template.\\nHow do we find the best degree? (Asking this question again in case some students only do R\\nThe main form of finding a good fit is to plot the model and see what it looks like visually. You simply\\ntest several degrees and you see which one gives you the best fit. The other option is to find the lowest\\nroot-mean-square error (RMSE) for your model, but in that case be careful not to overfit the data.\\n2.4 SVR\\n2.4.1 SVR Intuition\\nWhen should I use SVR?\\nYou should use SVR if a linear model like linear regression doesn’t fit very well your data. This would mean\\nyou are dealing with a non linear problem, where your data is not linearly distributed. Therefore in that\\ncase SVR could be a much better solution.\\nI didn’t understand the Intuition Lecture. Am I in trouble?\\nNot at all. SVR is a pretty abstract model and besides it is not that commonly used. What you must rather\\nunderstand is the SVM model, which you will see in Part 3 - Classification. Then once you understand the\\nSVM model, you will get a better grasp of the SVR model, since the SVR is simply the SVM for Regression.\\nHowever we wanted to include SVR in this course to give you an extra option in your Machine Learning\\ntoolkit.\\nPage 9 of 51\\nMachine Learning A-Z Q&A\\n2.4.2 SVR in Python\\nWhy do we need to ’sc_Y.inverse_transform’ ?\\nWe need the inverse_transform method to go back to the original scale. Indeed we applied feature scaling\\nso we get this scale around 0 and if we make a prediction without inversing the scale we will get the\\nscaled predicted salary. And of course we want the real salary, not the scaled one, so we have to use\\n’sc_Y.inverse_transform’. Also what is important to understand is that ’transform’ and ’inverse_transform’\\nare paired methods.\\n2.4.3 SVR in R\\nWhy did we not apply feature scaling like we did explicitly in Python\\nThat’s because in svm() function of R, the values are automatically scaled.\\nCan we select the most significant variables thanks to the p-value like we did in R before?\\nYou couldn’t use p-value because SVR is not a linear model, and p-values apply only to linear models.\\nTherefore feature selection is out of the question. But you could do feature extraction, which you will see in\\nPart 9 - Dimensionality Reduction. That you can apply to Decision Trees, and it will reduce the number of\\nyour features.\\n2.5 Decision Tree Regression\\n2.5.1 Decision Tree Regression Intuition\\nHow does the algorithm split the data points?\\nIt uses reduction of standard deviation of the predictions. In other words, the standard deviation is decreased\\nright after a split. Hence, building a decision tree is all about finding the attribute that returns the highest\\nstandard deviation reduction (i.e., the most homogeneous branches).\\nWhat is the Information Gain and how does it work in Decision Trees?\\nThe Information Gain in Decision Tree Regression is exactly the Standard Deviation Reduction we are\\nlooking to reach. We calculate by how much the Standard Deviation decreases after each split. Because the\\nmore the Standard Deviation is decreased after a split, the more homogeneous the child nodes will be.\\nWhat is the Entropy and how does it work in Decision Trees?\\nThe Entropy measures the disorder in a set, here in a part resulting from a split. So the more homogeneous\\nis your data in a part, the lower will be the entropy. The more you have splits, the more you have chance\\nto find parts in which your data is homogeneous, and therefore the lower will be the entropy (close to 0) in\\nthese parts. However you might still find some nodes where the data is not homogeneous, and therefore the\\nentropy would not be that small.\\n2.5.2 Decision Tree Regression in Python\\nDoes a Decision Tree make much sense in 1D?\\nNot really, as we saw in the practical part of this section. In 1D (meaning one independent variable), the\\nDecision Tree clearly tends to overfit the data. The Decision Tree would be much more relevant in higher\\ndimension, but keep in mind that the implementation we made here in 1D would be exactly the same in\\nhigher dimension. Therefore you might want to keep that model in your toolkit in case you are dealing\\nwith a higher dimensional space. This will actually be the case in Part 3 - Classification, where we will use\\nDecision Tree for Classification in 2D, which you will see turns out to be more relevant.\\nPage 10 of 51\\nMachine Learning A-Z Q&A\\n2.5.3 Decision Tree Regression in R\\nWhy do we get different results between Python and R?\\nThe difference is likely due to the random split of data. If we did a cross-validation (see Part 10) on all the\\nmodels in both languages, then you would likely get a similar mean accuracy. That being said, we would\\nrecommend more using Python for Decision Trees since the model is slightly better implemented in Python.\\nIs the Decision Tree appropriate here?\\nHere in this example, we can clearly see that the fitting curve is a stair with large gaps in the discontinuities.\\nThat decision tree regression model is therefore not the most appropriate, and that is because we have only\\none independent variables taking discrete values. So what happened is that the prediction was made in the\\nlower part of the gap in Python, and made in the upper part of the gap in R. And since the gap is large, that\\nmakes a big difference. If we had much more observations, taking values with more continuity (like with a\\n0.1 step), the gaps would be smaller and therefore the predictions in Python and R far from each other.\\nCan we select the most significant variables thanks to the p-value like we did in R before?\\nYou couldn’t use p-value because Decision Tree is not a linear model, and p-values apply only to linear\\nmodels. Therefore feature selection is out of the question. But you could do feature extraction, which you\\nwill see in Part 9 - Dimensionality Reduction. That you can apply to Decision Trees, and it will reduce the\\nnumber of your features.\\n2.6 Random Forest Regression\\n2.6.1 Random Forest Regression Intuition\\nWhat is the advantage and drawback of Random Forests compared to Decision Trees? Advan\\x02tage: Random Forests can give you a better predictive power than Decision Trees.\\nDrawback: Decision Tree will give you more interpretability than Random Forests, because you can plot the\\ngraph of a Decision Tree to see the different splits leading to the prediction, as seen in the Intuition Lecture.\\nThat’s something you can’t do with Random Forests.\\nWhen to use Random Forest and when to use the other models?\\nThe best answer to that question is: try them all!\\nIndeed, thanks to the templates it will only take you 10 minutes to try all the models, which is very little\\ncompared to the time dedicated to the other parts of a data science project (like Data Preprocessing for\\nexample). So just don’t be scared to try all the regression models and compare the results (through cross\\nvalidation which we will see in Part \\n',\n",
       " 'number': 8,\n",
       " 'subject': 'Machine Learning',\n",
       " 'tone': 'simple',\n",
       " 'response_json': '{\"1\": {\"mcq\": \"multiple choice question\", \"options\": {\"a\": \"choice here\", \"b\": \"choice here\", \"c\": \"choice here\", \"d\": \"choice here\"}, \"correct\": \"correct answer\"}, \"2\": {\"mcq\": \"multiple choice question\", \"options\": {\"a\": \"choice here\", \"b\": \"choice here\", \"c\": \"choice here\", \"d\": \"choice here\"}, \"correct\": \"correct answer\"}, \"3\": {\"mcq\": \"multiple choice question\", \"options\": {\"a\": \"choice here\", \"b\": \"choice here\", \"c\": \"choice here\", \"d\": \"choice here\"}, \"correct\": \"correct answer\"}}',\n",
       " 'quiz': '{\\n    \"1\": {\\n        \"mcq\": \"Is Polynomial Regression a linear or non linear model?\",\\n        \"options\": {\\n            \"a\": \"Linear\",\\n            \"b\": \"Non-linear\",\\n            \"c\": \"Both\",\\n            \"d\": \"None of the above\"\\n        },\\n        \"correct\": \"c\"\\n    },\\n    \"2\": {\\n        \"mcq\": \"Why didn’t we apply Feature Scaling in our Polynomial Regression model?\",\\n        \"options\": {\\n            \"a\": \"Feature Scaling is not necessary for Polynomial Regression\",\\n            \"b\": \"The coefficients can adapt their scale to put everything on the same scale\",\\n            \"c\": \"Feature Scaling would lead to overfitting\",\\n            \"d\": \"Feature Scaling is only applicable to non-linear models\"\\n        },\\n        \"correct\": \"b\"\\n    },\\n    \"3\": {\\n        \"mcq\": \"When should I use SVR?\",\\n        \"options\": {\\n            \"a\": \"When dealing with linear problems\",\\n            \"b\": \"When linear regression fits the data well\",\\n            \"c\": \"When dealing with non-linear problems\",\\n            \"d\": \"When the data is normally distributed\"\\n        },\\n        \"correct\": \"c\"\\n    },\\n    \"4\": {\\n        \"mcq\": \"What is the advantage of Random Forest compared to Decision Trees?\",\\n        \"options\": {\\n            \"a\": \"Random Forests provide better interpretability\",\\n            \"b\": \"Decision Trees have better predictive power\",\\n            \"c\": \"Random Forests can give better predictive power\",\\n            \"d\": \"Decision Trees are faster to train\"\\n        },\\n        \"correct\": \"c\"\\n    },\\n    \"5\": {\\n        \"mcq\": \"How do we find the best degree in Polynomial Regression?\",\\n        \"options\": {\\n            \"a\": \"By selecting the highest degree possible\",\\n            \"b\": \"By finding the lowest RMSE\",\\n            \"c\": \"By using p-values\",\\n            \"d\": \"By visual inspection and testing different degrees\"\\n        },\\n        \"correct\": \"d\"\\n    },\\n    \"6\": {\\n        \"mcq\": \"Why do we need to use \\'sc_Y.inverse_transform\\' in SVR?\",\\n        \"options\": {\\n            \"a\": \"To apply feature scaling\",\\n            \"b\": \"To make predictions on scaled data\",\\n            \"c\": \"To go back to the original scale\",\\n            \"d\": \"To improve model accuracy\"\\n        },\\n        \"correct\": \"c\"\\n    },\\n    \"7\": {\\n        \"mcq\": \"What is the Information Gain in Decision Trees?\",\\n        \"options\": {\\n            \"a\": \"A measure of disorder in a set\",\\n            \"b\": \"The reduction in standard deviation of predictions\",\\n            \"c\": \"The decrease in entropy after a split\",\\n            \"d\": \"The increase in variance after a split\"\\n        },\\n        \"correct\": \"b\"\\n    },\\n    \"8\": {\\n        \"mcq\": \"Can we select the most significant variables using p-values in Random Forest Regression?\",\\n        \"options\": {\\n            \"a\": \"Yes, p-values can be used for feature selection\",\\n            \"b\": \"No, p-values are not applicable to Random Forests\",\\n            \"c\": \"Random Forests do not require feature selection\",\\n            \"d\": \"Feature selection is done based on entropy\"\\n        },\\n        \"correct\": \"b\"\\n    }\\n}',\n",
       " 'review': '\\n{\\n    \"1\": {\\n        \"mcq\": \"What is Polynomial Regression considered as in terms of linearity?\",\\n        \"options\": {\\n            \"a\": \"Linear model\",\\n            \"b\": \"Non-linear model\",\\n            \"c\": \"Both linear and non-linear model\",\\n            \"d\": \"None of the above\"\\n        },\\n        \"correct\": \"c\"\\n    },\\n    \"2\": {\\n        \"mcq\": \"When should SVR be used?\",\\n        \"options\": {\\n            \"a\": \"When dealing with linear problems\",\\n            \"b\": \"When linear regression fits the data well\",\\n            \"c\": \"When dealing with non-linear problems\",\\n            \"d\": \"When data is linearly distributed\"\\n        },\\n        \"correct\": \"c\"\\n    },\\n    \"3\": {\\n        \"mcq\": \"What is the main form of finding the best degree in Polynomial Regression?\",\\n        \"options\": {\\n            \"a\": \"Using p-values\",\\n            \"b\": \"Plotting the model visually\",\\n            \"c\": \"Applying feature scaling\",\\n            \"d\": \"Using cross-validation\"\\n        },\\n        \"correct\": \"b\"\\n    },\\n    \"4\": {\\n        \"mcq\": \"What is the advantage of Random Forests over Decision Trees?\",\\n        \"options\": {\\n            \"a\": \"Better interpretability\",\\n            \"b\": \"Higher predictive power\",\\n            \"c\": \"Ability to plot the graph\",\\n            \"d\": \"Faster computation\"\\n        },\\n        \"correct\": \"b\"\\n    },\\n    \"5\": {\\n        \"mcq\": \"Why do we need to apply Feature Scaling in Polynomial Regression?\",\\n        \"options\": {\\n            \"a\": \"To make the model more complex\",\\n            \"b\": \"To speed up the computation\",\\n            \"c\": \"To put all variables on the same scale\",\\n            \"d\": \"To reduce the number of features\"\\n        },\\n        \"correct\": \"c\"\\n    },\\n    \"6\": {\\n        \"mcq\": \"How does a Decision Tree split the data points?\",\\n        \"options\": {\\n            \"a\": \"By maximizing the mean\",\\n            \"b\": \"By reducing the standard deviation of predictions\",\\n            \"c\": \"By minimizing the variance\",\\n            \"d\": \"By increasing the entropy\"\\n        },\\n        \"correct\": \"b\"\\n    },\\n    \"7\": {\\n        \"mcq\": \"Why is SVR considered a non-linear model?\",\\n        \"options\": {\\n            \"a\": \"Because it has linear coefficients\",\\n            \"b\": \"Because it uses polynomial features\",\\n            \"c\": \"Because it is a variant of SVM\",\\n            \"d\": \"Because it deals with non-linearly distributed data\"\\n        },\\n        \"correct\": \"d\"\\n    },\\n    \"8\": {\\n        \"mcq\": \"Can p-values be used for feature selection in Decision Trees?\",\\n        \"options\": {\\n            \"a\": \"Yes, always\",\\n            \"b\": \"No, p-values are not applicable\",\\n            \"c\": \"Only in R but not in Python\",\\n            \"d\": \"Depends on the number of features\"\\n        },\\n        \"correct\": \"b\"\\n    }\\n}'}"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'{\\n    \"1\": {\\n        \"mcq\": \"Is Polynomial Regression a linear or non linear model?\",\\n        \"options\": {\\n            \"a\": \"Linear\",\\n            \"b\": \"Non-linear\",\\n            \"c\": \"Both\",\\n            \"d\": \"None of the above\"\\n        },\\n        \"correct\": \"c\"\\n    },\\n    \"2\": {\\n        \"mcq\": \"Why didn’t we apply Feature Scaling in our Polynomial Regression model?\",\\n        \"options\": {\\n            \"a\": \"Feature Scaling is not necessary for Polynomial Regression\",\\n            \"b\": \"The coefficients can adapt their scale to put everything on the same scale\",\\n            \"c\": \"Feature Scaling would lead to overfitting\",\\n            \"d\": \"Feature Scaling is only applicable to non-linear models\"\\n        },\\n        \"correct\": \"b\"\\n    },\\n    \"3\": {\\n        \"mcq\": \"When should I use SVR?\",\\n        \"options\": {\\n            \"a\": \"When dealing with linear problems\",\\n            \"b\": \"When linear regression fits the data well\",\\n            \"c\": \"When dealing with non-linear problems\",\\n            \"d\": \"When the data is normally distributed\"\\n        },\\n        \"correct\": \"c\"\\n    },\\n    \"4\": {\\n        \"mcq\": \"What is the advantage of Random Forest compared to Decision Trees?\",\\n        \"options\": {\\n            \"a\": \"Random Forests provide better interpretability\",\\n            \"b\": \"Decision Trees have better predictive power\",\\n            \"c\": \"Random Forests can give better predictive power\",\\n            \"d\": \"Decision Trees are faster to train\"\\n        },\\n        \"correct\": \"c\"\\n    },\\n    \"5\": {\\n        \"mcq\": \"How do we find the best degree in Polynomial Regression?\",\\n        \"options\": {\\n            \"a\": \"By selecting the highest degree possible\",\\n            \"b\": \"By finding the lowest RMSE\",\\n            \"c\": \"By using p-values\",\\n            \"d\": \"By visual inspection and testing different degrees\"\\n        },\\n        \"correct\": \"d\"\\n    },\\n    \"6\": {\\n        \"mcq\": \"Why do we need to use \\'sc_Y.inverse_transform\\' in SVR?\",\\n        \"options\": {\\n            \"a\": \"To apply feature scaling\",\\n            \"b\": \"To make predictions on scaled data\",\\n            \"c\": \"To go back to the original scale\",\\n            \"d\": \"To improve model accuracy\"\\n        },\\n        \"correct\": \"c\"\\n    },\\n    \"7\": {\\n        \"mcq\": \"What is the Information Gain in Decision Trees?\",\\n        \"options\": {\\n            \"a\": \"A measure of disorder in a set\",\\n            \"b\": \"The reduction in standard deviation of predictions\",\\n            \"c\": \"The decrease in entropy after a split\",\\n            \"d\": \"The increase in variance after a split\"\\n        },\\n        \"correct\": \"b\"\\n    },\\n    \"8\": {\\n        \"mcq\": \"Can we select the most significant variables using p-values in Random Forest Regression?\",\\n        \"options\": {\\n            \"a\": \"Yes, p-values can be used for feature selection\",\\n            \"b\": \"No, p-values are not applicable to Random Forests\",\\n            \"c\": \"Random Forests do not require feature selection\",\\n            \"d\": \"Feature selection is done based on entropy\"\\n        },\\n        \"correct\": \"b\"\\n    }\\n}'"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response.get(\"quiz\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "quiz=response.get(\"quiz\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parse the JSON string to a dictionary \n",
    "quiz = json.loads(quiz) # Access the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'\\n{\\n    \"1\": {\\n        \"mcq\": \"What is Polynomial Regression considered as in terms of linearity?\",\\n        \"options\": {\\n            \"a\": \"Linear model\",\\n            \"b\": \"Non-linear model\",\\n            \"c\": \"Both linear and non-linear model\",\\n            \"d\": \"None of the above\"\\n        },\\n        \"correct\": \"c\"\\n    },\\n    \"2\": {\\n        \"mcq\": \"When should SVR be used?\",\\n        \"options\": {\\n            \"a\": \"When dealing with linear problems\",\\n            \"b\": \"When linear regression fits the data well\",\\n            \"c\": \"When dealing with non-linear problems\",\\n            \"d\": \"When data is linearly distributed\"\\n        },\\n        \"correct\": \"c\"\\n    },\\n    \"3\": {\\n        \"mcq\": \"What is the main form of finding the best degree in Polynomial Regression?\",\\n        \"options\": {\\n            \"a\": \"Using p-values\",\\n            \"b\": \"Plotting the model visually\",\\n            \"c\": \"Applying feature scaling\",\\n            \"d\": \"Using cross-validation\"\\n        },\\n        \"correct\": \"b\"\\n    },\\n    \"4\": {\\n        \"mcq\": \"What is the advantage of Random Forests over Decision Trees?\",\\n        \"options\": {\\n            \"a\": \"Better interpretability\",\\n            \"b\": \"Higher predictive power\",\\n            \"c\": \"Ability to plot the graph\",\\n            \"d\": \"Faster computation\"\\n        },\\n        \"correct\": \"b\"\\n    },\\n    \"5\": {\\n        \"mcq\": \"Why do we need to apply Feature Scaling in Polynomial Regression?\",\\n        \"options\": {\\n            \"a\": \"To make the model more complex\",\\n            \"b\": \"To speed up the computation\",\\n            \"c\": \"To put all variables on the same scale\",\\n            \"d\": \"To reduce the number of features\"\\n        },\\n        \"correct\": \"c\"\\n    },\\n    \"6\": {\\n        \"mcq\": \"How does a Decision Tree split the data points?\",\\n        \"options\": {\\n            \"a\": \"By maximizing the mean\",\\n            \"b\": \"By reducing the standard deviation of predictions\",\\n            \"c\": \"By minimizing the variance\",\\n            \"d\": \"By increasing the entropy\"\\n        },\\n        \"correct\": \"b\"\\n    },\\n    \"7\": {\\n        \"mcq\": \"Why is SVR considered a non-linear model?\",\\n        \"options\": {\\n            \"a\": \"Because it has linear coefficients\",\\n            \"b\": \"Because it uses polynomial features\",\\n            \"c\": \"Because it is a variant of SVM\",\\n            \"d\": \"Because it deals with non-linearly distributed data\"\\n        },\\n        \"correct\": \"d\"\\n    },\\n    \"8\": {\\n        \"mcq\": \"Can p-values be used for feature selection in Decision Trees?\",\\n        \"options\": {\\n            \"a\": \"Yes, always\",\\n            \"b\": \"No, p-values are not applicable\",\\n            \"c\": \"Only in R but not in Python\",\\n            \"d\": \"Depends on the number of features\"\\n        },\\n        \"correct\": \"b\"\\n    }\\n}'}"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "{response[\"review\"]}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'text': 'Polynomial Regression\\n2.3.1 Polynomial Regression Intuition\\nIs Polynomial Regression a linear or non linear model?\\nThat depends on what you are referring to. Polynomial Regression is linear on the coefficients since we don’t\\nhave any power of the coefficients (all the coefficients are raised to the power of 1: b0, b1, ..., bn). However,\\nPolynomial Regression is a non linear function of the input x, since we have the inputs raised to several\\npowers: x (power 1), x\\n2\\n(power 2), ..., x\\nn (power n). That is how we can also see the Polynomial Regression\\nas a non linear model. Besides indeed, Polynomial Regression is appropriate when the data is non linearly\\ndistributed (meaning you can’t fit a straight line between y and x).\\nPage 8 of 51\\nMachine Learning A-Z Q&A\\n2.3.2 Polynomial Regression in Python\\nWhy didn’t we apply Feature Scaling in our Polynomial Regression model?\\nIt’s simply because, since y is a linear combination of x and x\\n2\\n, the coefficients can adapt their scale to put\\neverything on the same scale. For example if y takes values between 0 and 1, x takes values between 1 and\\n10 and x\\n2\\ntakes values between 1 and 100, then b1 can be multiplied by 0.1 and b2 can be multiplied by 0.01\\nso that y, b1x1 and b2x2 are all on the same scale.\\nHow do we find the best degree?\\nThe main form of finding a good fit is to plot the model and see what it looks like visually. You simply\\ntest several degrees and you see which one gives you the best fit. The other option is to find the lowest\\nroot-mean-square error (RMSE) for your model, but in that case be careful not to overfit the data.\\nWhy did we have to create a second linear regressor ’lin_reg_2’ ? Could we not have used\\n’lin_reg’ directly?\\nNo, because ’lin_reg’ is already fitted to X and y and now we want to fit a new linear model to Xpoly and\\ny. So we have to create a new regressor object. One must important that the fit method here finds the\\ncoefficient between the independent variables and the dependent variable. Therefore since ’lin_reg’ already\\ngot the coefficients of correlation between X and y, ’lin_reg_2’ has to be created to get some new coefficients\\nof correlations between Xpoly and y.\\n2.3.3 Polynomial Regression in R\\nIn R, I have to create manually the different columns for each polynomial feature? What if\\nthere are many polynomial features?\\nYou will very rarely have to create more than 4 polynomial features, otherwise you would get overfitting,\\nwhich must absolutely be avoided in Machine Learning. So even if you have to create them manually, it will\\nnever take too much of your time. Besides you can use the template.\\nHow do we find the best degree? (Asking this question again in case some students only do R\\nThe main form of finding a good fit is to plot the model and see what it looks like visually. You simply\\ntest several degrees and you see which one gives you the best fit. The other option is to find the lowest\\nroot-mean-square error (RMSE) for your model, but in that case be careful not to overfit the data.\\n2.4 SVR\\n2.4.1 SVR Intuition\\nWhen should I use SVR?\\nYou should use SVR if a linear model like linear regression doesn’t fit very well your data. This would mean\\nyou are dealing with a non linear problem, where your data is not linearly distributed. Therefore in that\\ncase SVR could be a much better solution.\\nI didn’t understand the Intuition Lecture. Am I in trouble?\\nNot at all. SVR is a pretty abstract model and besides it is not that commonly used. What you must rather\\nunderstand is the SVM model, which you will see in Part 3 - Classification. Then once you understand the\\nSVM model, you will get a better grasp of the SVR model, since the SVR is simply the SVM for Regression.\\nHowever we wanted to include SVR in this course to give you an extra option in your Machine Learning\\ntoolkit.\\nPage 9 of 51\\nMachine Learning A-Z Q&A\\n2.4.2 SVR in Python\\nWhy do we need to ’sc_Y.inverse_transform’ ?\\nWe need the inverse_transform method to go back to the original scale. Indeed we applied feature scaling\\nso we get this scale around 0 and if we make a prediction without inversing the scale we will get the\\nscaled predicted salary. And of course we want the real salary, not the scaled one, so we have to use\\n’sc_Y.inverse_transform’. Also what is important to understand is that ’transform’ and ’inverse_transform’\\nare paired methods.\\n2.4.3 SVR in R\\nWhy did we not apply feature scaling like we did explicitly in Python\\nThat’s because in svm() function of R, the values are automatically scaled.\\nCan we select the most significant variables thanks to the p-value like we did in R before?\\nYou couldn’t use p-value because SVR is not a linear model, and p-values apply only to linear models.\\nTherefore feature selection is out of the question. But you could do feature extraction, which you will see in\\nPart 9 - Dimensionality Reduction. That you can apply to Decision Trees, and it will reduce the number of\\nyour features.\\n2.5 Decision Tree Regression\\n2.5.1 Decision Tree Regression Intuition\\nHow does the algorithm split the data points?\\nIt uses reduction of standard deviation of the predictions. In other words, the standard deviation is decreased\\nright after a split. Hence, building a decision tree is all about finding the attribute that returns the highest\\nstandard deviation reduction (i.e., the most homogeneous branches).\\nWhat is the Information Gain and how does it work in Decision Trees?\\nThe Information Gain in Decision Tree Regression is exactly the Standard Deviation Reduction we are\\nlooking to reach. We calculate by how much the Standard Deviation decreases after each split. Because the\\nmore the Standard Deviation is decreased after a split, the more homogeneous the child nodes will be.\\nWhat is the Entropy and how does it work in Decision Trees?\\nThe Entropy measures the disorder in a set, here in a part resulting from a split. So the more homogeneous\\nis your data in a part, the lower will be the entropy. The more you have splits, the more you have chance\\nto find parts in which your data is homogeneous, and therefore the lower will be the entropy (close to 0) in\\nthese parts. However you might still find some nodes where the data is not homogeneous, and therefore the\\nentropy would not be that small.\\n2.5.2 Decision Tree Regression in Python\\nDoes a Decision Tree make much sense in 1D?\\nNot really, as we saw in the practical part of this section. In 1D (meaning one independent variable), the\\nDecision Tree clearly tends to overfit the data. The Decision Tree would be much more relevant in higher\\ndimension, but keep in mind that the implementation we made here in 1D would be exactly the same in\\nhigher dimension. Therefore you might want to keep that model in your toolkit in case you are dealing\\nwith a higher dimensional space. This will actually be the case in Part 3 - Classification, where we will use\\nDecision Tree for Classification in 2D, which you will see turns out to be more relevant.\\nPage 10 of 51\\nMachine Learning A-Z Q&A\\n2.5.3 Decision Tree Regression in R\\nWhy do we get different results between Python and R?\\nThe difference is likely due to the random split of data. If we did a cross-validation (see Part 10) on all the\\nmodels in both languages, then you would likely get a similar mean accuracy. That being said, we would\\nrecommend more using Python for Decision Trees since the model is slightly better implemented in Python.\\nIs the Decision Tree appropriate here?\\nHere in this example, we can clearly see that the fitting curve is a stair with large gaps in the discontinuities.\\nThat decision tree regression model is therefore not the most appropriate, and that is because we have only\\none independent variables taking discrete values. So what happened is that the prediction was made in the\\nlower part of the gap in Python, and made in the upper part of the gap in R. And since the gap is large, that\\nmakes a big difference. If we had much more observations, taking values with more continuity (like with a\\n0.1 step), the gaps would be smaller and therefore the predictions in Python and R far from each other.\\nCan we select the most significant variables thanks to the p-value like we did in R before?\\nYou couldn’t use p-value because Decision Tree is not a linear model, and p-values apply only to linear\\nmodels. Therefore feature selection is out of the question. But you could do feature extraction, which you\\nwill see in Part 9 - Dimensionality Reduction. That you can apply to Decision Trees, and it will reduce the\\nnumber of your features.\\n2.6 Random Forest Regression\\n2.6.1 Random Forest Regression Intuition\\nWhat is the advantage and drawback of Random Forests compared to Decision Trees? Advan\\x02tage: Random Forests can give you a better predictive power than Decision Trees.\\nDrawback: Decision Tree will give you more interpretability than Random Forests, because you can plot the\\ngraph of a Decision Tree to see the different splits leading to the prediction, as seen in the Intuition Lecture.\\nThat’s something you can’t do with Random Forests.\\nWhen to use Random Forest and when to use the other models?\\nThe best answer to that question is: try them all!\\nIndeed, thanks to the templates it will only take you 10 minutes to try all the models, which is very little\\ncompared to the time dedicated to the other parts of a data science project (like Data Preprocessing for\\nexample). So just don’t be scared to try all the regression models and compare the results (through cross\\nvalidation which we will see in Part \\n',\n",
       " 'number': 8,\n",
       " 'subject': 'Machine Learning',\n",
       " 'tone': 'simple',\n",
       " 'response_json': '{\"1\": {\"mcq\": \"multiple choice question\", \"options\": {\"a\": \"choice here\", \"b\": \"choice here\", \"c\": \"choice here\", \"d\": \"choice here\"}, \"correct\": \"correct answer\"}, \"2\": {\"mcq\": \"multiple choice question\", \"options\": {\"a\": \"choice here\", \"b\": \"choice here\", \"c\": \"choice here\", \"d\": \"choice here\"}, \"correct\": \"correct answer\"}, \"3\": {\"mcq\": \"multiple choice question\", \"options\": {\"a\": \"choice here\", \"b\": \"choice here\", \"c\": \"choice here\", \"d\": \"choice here\"}, \"correct\": \"correct answer\"}}',\n",
       " 'quiz': '{\\n    \"1\": {\\n        \"mcq\": \"Is Polynomial Regression a linear or non linear model?\",\\n        \"options\": {\\n            \"a\": \"Linear\",\\n            \"b\": \"Non-linear\",\\n            \"c\": \"Both\",\\n            \"d\": \"None of the above\"\\n        },\\n        \"correct\": \"c\"\\n    },\\n    \"2\": {\\n        \"mcq\": \"Why didn’t we apply Feature Scaling in our Polynomial Regression model?\",\\n        \"options\": {\\n            \"a\": \"Feature Scaling is not necessary for Polynomial Regression\",\\n            \"b\": \"The coefficients can adapt their scale to put everything on the same scale\",\\n            \"c\": \"Feature Scaling would lead to overfitting\",\\n            \"d\": \"Feature Scaling is only applicable to non-linear models\"\\n        },\\n        \"correct\": \"b\"\\n    },\\n    \"3\": {\\n        \"mcq\": \"When should I use SVR?\",\\n        \"options\": {\\n            \"a\": \"When dealing with linear problems\",\\n            \"b\": \"When linear regression fits the data well\",\\n            \"c\": \"When dealing with non-linear problems\",\\n            \"d\": \"When the data is normally distributed\"\\n        },\\n        \"correct\": \"c\"\\n    },\\n    \"4\": {\\n        \"mcq\": \"What is the advantage of Random Forest compared to Decision Trees?\",\\n        \"options\": {\\n            \"a\": \"Random Forests provide better interpretability\",\\n            \"b\": \"Decision Trees have better predictive power\",\\n            \"c\": \"Random Forests can give better predictive power\",\\n            \"d\": \"Decision Trees are faster to train\"\\n        },\\n        \"correct\": \"c\"\\n    },\\n    \"5\": {\\n        \"mcq\": \"How do we find the best degree in Polynomial Regression?\",\\n        \"options\": {\\n            \"a\": \"By selecting the highest degree possible\",\\n            \"b\": \"By finding the lowest RMSE\",\\n            \"c\": \"By using p-values\",\\n            \"d\": \"By visual inspection and testing different degrees\"\\n        },\\n        \"correct\": \"d\"\\n    },\\n    \"6\": {\\n        \"mcq\": \"Why do we need to use \\'sc_Y.inverse_transform\\' in SVR?\",\\n        \"options\": {\\n            \"a\": \"To apply feature scaling\",\\n            \"b\": \"To make predictions on scaled data\",\\n            \"c\": \"To go back to the original scale\",\\n            \"d\": \"To improve model accuracy\"\\n        },\\n        \"correct\": \"c\"\\n    },\\n    \"7\": {\\n        \"mcq\": \"What is the Information Gain in Decision Trees?\",\\n        \"options\": {\\n            \"a\": \"A measure of disorder in a set\",\\n            \"b\": \"The reduction in standard deviation of predictions\",\\n            \"c\": \"The decrease in entropy after a split\",\\n            \"d\": \"The increase in variance after a split\"\\n        },\\n        \"correct\": \"b\"\\n    },\\n    \"8\": {\\n        \"mcq\": \"Can we select the most significant variables using p-values in Random Forest Regression?\",\\n        \"options\": {\\n            \"a\": \"Yes, p-values can be used for feature selection\",\\n            \"b\": \"No, p-values are not applicable to Random Forests\",\\n            \"c\": \"Random Forests do not require feature selection\",\\n            \"d\": \"Feature selection is done based on entropy\"\\n        },\\n        \"correct\": \"b\"\\n    }\\n}',\n",
       " 'review': '\\n{\\n    \"1\": {\\n        \"mcq\": \"What is Polynomial Regression considered as in terms of linearity?\",\\n        \"options\": {\\n            \"a\": \"Linear model\",\\n            \"b\": \"Non-linear model\",\\n            \"c\": \"Both linear and non-linear model\",\\n            \"d\": \"None of the above\"\\n        },\\n        \"correct\": \"c\"\\n    },\\n    \"2\": {\\n        \"mcq\": \"When should SVR be used?\",\\n        \"options\": {\\n            \"a\": \"When dealing with linear problems\",\\n            \"b\": \"When linear regression fits the data well\",\\n            \"c\": \"When dealing with non-linear problems\",\\n            \"d\": \"When data is linearly distributed\"\\n        },\\n        \"correct\": \"c\"\\n    },\\n    \"3\": {\\n        \"mcq\": \"What is the main form of finding the best degree in Polynomial Regression?\",\\n        \"options\": {\\n            \"a\": \"Using p-values\",\\n            \"b\": \"Plotting the model visually\",\\n            \"c\": \"Applying feature scaling\",\\n            \"d\": \"Using cross-validation\"\\n        },\\n        \"correct\": \"b\"\\n    },\\n    \"4\": {\\n        \"mcq\": \"What is the advantage of Random Forests over Decision Trees?\",\\n        \"options\": {\\n            \"a\": \"Better interpretability\",\\n            \"b\": \"Higher predictive power\",\\n            \"c\": \"Ability to plot the graph\",\\n            \"d\": \"Faster computation\"\\n        },\\n        \"correct\": \"b\"\\n    },\\n    \"5\": {\\n        \"mcq\": \"Why do we need to apply Feature Scaling in Polynomial Regression?\",\\n        \"options\": {\\n            \"a\": \"To make the model more complex\",\\n            \"b\": \"To speed up the computation\",\\n            \"c\": \"To put all variables on the same scale\",\\n            \"d\": \"To reduce the number of features\"\\n        },\\n        \"correct\": \"c\"\\n    },\\n    \"6\": {\\n        \"mcq\": \"How does a Decision Tree split the data points?\",\\n        \"options\": {\\n            \"a\": \"By maximizing the mean\",\\n            \"b\": \"By reducing the standard deviation of predictions\",\\n            \"c\": \"By minimizing the variance\",\\n            \"d\": \"By increasing the entropy\"\\n        },\\n        \"correct\": \"b\"\\n    },\\n    \"7\": {\\n        \"mcq\": \"Why is SVR considered a non-linear model?\",\\n        \"options\": {\\n            \"a\": \"Because it has linear coefficients\",\\n            \"b\": \"Because it uses polynomial features\",\\n            \"c\": \"Because it is a variant of SVM\",\\n            \"d\": \"Because it deals with non-linearly distributed data\"\\n        },\\n        \"correct\": \"d\"\\n    },\\n    \"8\": {\\n        \"mcq\": \"Can p-values be used for feature selection in Decision Trees?\",\\n        \"options\": {\\n            \"a\": \"Yes, always\",\\n            \"b\": \"No, p-values are not applicable\",\\n            \"c\": \"Only in R but not in Python\",\\n            \"d\": \"Depends on the number of features\"\\n        },\\n        \"correct\": \"b\"\\n    }\\n}'}"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'1': {'mcq': 'Is Polynomial Regression a linear or non linear model?', 'options': {'a': 'Linear', 'b': 'Non-linear', 'c': 'Both', 'd': 'None of the above'}, 'correct': 'c'}, '2': {'mcq': 'Why didn’t we apply Feature Scaling in our Polynomial Regression model?', 'options': {'a': 'Feature Scaling is not necessary for Polynomial Regression', 'b': 'The coefficients can adapt their scale to put everything on the same scale', 'c': 'Feature Scaling would lead to overfitting', 'd': 'Feature Scaling is only applicable to non-linear models'}, 'correct': 'b'}, '3': {'mcq': 'When should I use SVR?', 'options': {'a': 'When dealing with linear problems', 'b': 'When linear regression fits the data well', 'c': 'When dealing with non-linear problems', 'd': 'When the data is normally distributed'}, 'correct': 'c'}, '4': {'mcq': 'What is the advantage of Random Forest compared to Decision Trees?', 'options': {'a': 'Random Forests provide better interpretability', 'b': 'Decision Trees have better predictive power', 'c': 'Random Forests can give better predictive power', 'd': 'Decision Trees are faster to train'}, 'correct': 'c'}, '5': {'mcq': 'How do we find the best degree in Polynomial Regression?', 'options': {'a': 'By selecting the highest degree possible', 'b': 'By finding the lowest RMSE', 'c': 'By using p-values', 'd': 'By visual inspection and testing different degrees'}, 'correct': 'd'}, '6': {'mcq': \"Why do we need to use 'sc_Y.inverse_transform' in SVR?\", 'options': {'a': 'To apply feature scaling', 'b': 'To make predictions on scaled data', 'c': 'To go back to the original scale', 'd': 'To improve model accuracy'}, 'correct': 'c'}, '7': {'mcq': 'What is the Information Gain in Decision Trees?', 'options': {'a': 'A measure of disorder in a set', 'b': 'The reduction in standard deviation of predictions', 'c': 'The decrease in entropy after a split', 'd': 'The increase in variance after a split'}, 'correct': 'b'}, '8': {'mcq': 'Can we select the most significant variables using p-values in Random Forest Regression?', 'options': {'a': 'Yes, p-values can be used for feature selection', 'b': 'No, p-values are not applicable to Random Forests', 'c': 'Random Forests do not require feature selection', 'd': 'Feature selection is done based on entropy'}, 'correct': 'b'}}\n",
      "<class 'dict'>\n"
     ]
    }
   ],
   "source": [
    "print(quiz)\n",
    "print(type(quiz))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "quiz_table_data = []\n",
    "for key, value in quiz.items():\n",
    "    mcq = value[\"mcq\"]\n",
    "    options = \" | \".join(\n",
    "        [\n",
    "            f\"{option}: {option_value}\"\n",
    "            for option, option_value in value[\"options\"].items()\n",
    "            ]\n",
    "        )\n",
    "    correct = value[\"correct\"]\n",
    "    quiz_table_data.append({\"MCQ\": mcq, \"Choices\": options, \"Correct\": correct})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'MCQ': 'Is Polynomial Regression a linear or non linear model?',\n",
       "  'Choices': 'a: Linear | b: Non-linear | c: Both | d: None of the above',\n",
       "  'Correct': 'c'},\n",
       " {'MCQ': 'Why didn’t we apply Feature Scaling in our Polynomial Regression model?',\n",
       "  'Choices': 'a: Feature Scaling is not necessary for Polynomial Regression | b: The coefficients can adapt their scale to put everything on the same scale | c: Feature Scaling would lead to overfitting | d: Feature Scaling is only applicable to non-linear models',\n",
       "  'Correct': 'b'},\n",
       " {'MCQ': 'When should I use SVR?',\n",
       "  'Choices': 'a: When dealing with linear problems | b: When linear regression fits the data well | c: When dealing with non-linear problems | d: When the data is normally distributed',\n",
       "  'Correct': 'c'},\n",
       " {'MCQ': 'What is the advantage of Random Forest compared to Decision Trees?',\n",
       "  'Choices': 'a: Random Forests provide better interpretability | b: Decision Trees have better predictive power | c: Random Forests can give better predictive power | d: Decision Trees are faster to train',\n",
       "  'Correct': 'c'},\n",
       " {'MCQ': 'How do we find the best degree in Polynomial Regression?',\n",
       "  'Choices': 'a: By selecting the highest degree possible | b: By finding the lowest RMSE | c: By using p-values | d: By visual inspection and testing different degrees',\n",
       "  'Correct': 'd'},\n",
       " {'MCQ': \"Why do we need to use 'sc_Y.inverse_transform' in SVR?\",\n",
       "  'Choices': 'a: To apply feature scaling | b: To make predictions on scaled data | c: To go back to the original scale | d: To improve model accuracy',\n",
       "  'Correct': 'c'},\n",
       " {'MCQ': 'What is the Information Gain in Decision Trees?',\n",
       "  'Choices': 'a: A measure of disorder in a set | b: The reduction in standard deviation of predictions | c: The decrease in entropy after a split | d: The increase in variance after a split',\n",
       "  'Correct': 'b'},\n",
       " {'MCQ': 'Can we select the most significant variables using p-values in Random Forest Regression?',\n",
       "  'Choices': 'a: Yes, p-values can be used for feature selection | b: No, p-values are not applicable to Random Forests | c: Random Forests do not require feature selection | d: Feature selection is done based on entropy',\n",
       "  'Correct': 'b'}]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "quiz_table_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "list"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(quiz_table_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>MCQ</th>\n",
       "      <th>Choices</th>\n",
       "      <th>Correct</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Is Polynomial Regression a linear or non linea...</td>\n",
       "      <td>a: Linear | b: Non-linear | c: Both | d: None ...</td>\n",
       "      <td>c</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Why didn’t we apply Feature Scaling in our Pol...</td>\n",
       "      <td>a: Feature Scaling is not necessary for Polyno...</td>\n",
       "      <td>b</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>When should I use SVR?</td>\n",
       "      <td>a: When dealing with linear problems | b: When...</td>\n",
       "      <td>c</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>What is the advantage of Random Forest compare...</td>\n",
       "      <td>a: Random Forests provide better interpretabil...</td>\n",
       "      <td>c</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>How do we find the best degree in Polynomial R...</td>\n",
       "      <td>a: By selecting the highest degree possible | ...</td>\n",
       "      <td>d</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Why do we need to use 'sc_Y.inverse_transform'...</td>\n",
       "      <td>a: To apply feature scaling | b: To make predi...</td>\n",
       "      <td>c</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>What is the Information Gain in Decision Trees?</td>\n",
       "      <td>a: A measure of disorder in a set | b: The red...</td>\n",
       "      <td>b</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Can we select the most significant variables u...</td>\n",
       "      <td>a: Yes, p-values can be used for feature selec...</td>\n",
       "      <td>b</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 MCQ  \\\n",
       "0  Is Polynomial Regression a linear or non linea...   \n",
       "1  Why didn’t we apply Feature Scaling in our Pol...   \n",
       "2                             When should I use SVR?   \n",
       "3  What is the advantage of Random Forest compare...   \n",
       "4  How do we find the best degree in Polynomial R...   \n",
       "5  Why do we need to use 'sc_Y.inverse_transform'...   \n",
       "6    What is the Information Gain in Decision Trees?   \n",
       "7  Can we select the most significant variables u...   \n",
       "\n",
       "                                             Choices Correct  \n",
       "0  a: Linear | b: Non-linear | c: Both | d: None ...       c  \n",
       "1  a: Feature Scaling is not necessary for Polyno...       b  \n",
       "2  a: When dealing with linear problems | b: When...       c  \n",
       "3  a: Random Forests provide better interpretabil...       c  \n",
       "4  a: By selecting the highest degree possible | ...       d  \n",
       "5  a: To apply feature scaling | b: To make predi...       c  \n",
       "6  a: A measure of disorder in a set | b: The red...       b  \n",
       "7  a: Yes, p-values can be used for feature selec...       b  "
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(quiz_table_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'dict' object has no attribute 'to_csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[38], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mquiz\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto_csv\u001b[49m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmachinelearning.csv\u001b[39m\u001b[38;5;124m\"\u001b[39m,index\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'dict' object has no attribute 'to_csv'"
     ]
    }
   ],
   "source": [
    "quiz.to_csv(\"machinelearning.csv\",index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
