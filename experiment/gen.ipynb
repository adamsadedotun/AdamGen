{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 626,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import pandas as pd\n",
    "import traceback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 627,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 628,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Directory created at C:\\WINDOWS\\TEMP\\tmpcmts5mn7\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "# Path to create\n",
    "directory_path = r\"C:\\WINDOWS\\TEMP\\tmpcmts5mn7\"\n",
    "\n",
    "# Create the directory\n",
    "os.makedirs(directory_path, exist_ok=True)\n",
    "print(f\"Directory created at {directory_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 629,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 629,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()  # take environment variables from .env."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 630,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chat_models import ChatOpenAI\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 631,
   "metadata": {},
   "outputs": [],
   "source": [
    "mykey=os.getenv(\"OpenAI_Key\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 632,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = ChatOpenAI(openai_api_key=mykey, model=\"gpt-3.5-turbo\", temperature=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 633,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['LangChainDeprecationWarning', '__all__', '__builtins__', '__cached__', '__doc__', '__file__', '__getattr__', '__loader__', '__name__', '__package__', '__path__', '__spec__', 'base', 'init_chat_model', 'is_interactive_env', 'warnings']\n"
     ]
    }
   ],
   "source": [
    "import langchain.chat_models\n",
    "print(dir(langchain.chat_models))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 634,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "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\n"
     ]
    }
   ],
   "source": [
    "print(mykey)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 635,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.llms import OpenAI\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.chains import LLMChain\n",
    "from langchain.chains import SequentialChain\n",
    "from langchain.callbacks import get_openai_callback\n",
    "import PyPDF2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 636,
   "metadata": {},
   "outputs": [],
   "source": [
    "RESPONSE_JSON = {\n",
    "    \"1\": {\n",
    "        \"mcq\": \"multiple choice question\",\n",
    "        \"options\": {\n",
    "            \"a\": \"choice here\",\n",
    "            \"b\": \"choice here\",\n",
    "            \"c\": \"choice here\",\n",
    "            \"d\": \"choice here\",\n",
    "        },\n",
    "        \"correct\": \"correct answer\",\n",
    "    },\n",
    "    \"2\": {\n",
    "        \"mcq\": \"multiple choice question\",\n",
    "        \"options\": {\n",
    "            \"a\": \"choice here\",\n",
    "            \"b\": \"choice here\",\n",
    "            \"c\": \"choice here\",\n",
    "            \"d\": \"choice here\",\n",
    "        },\n",
    "        \"correct\": \"correct answer\",\n",
    "    },\n",
    "    \"3\": {\n",
    "        \"mcq\": \"multiple choice question\",\n",
    "        \"options\": {\n",
    "            \"a\": \"choice here\",\n",
    "            \"b\": \"choice here\",\n",
    "            \"c\": \"choice here\",\n",
    "            \"d\": \"choice here\",\n",
    "        },\n",
    "        \"correct\": \"correct answer\",\n",
    "    },\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 638,
   "metadata": {},
   "outputs": [],
   "source": [
    "TEMPLATE=\"\"\"\n",
    "Text:{text}\n",
    "You are an expert MCQ maker. Given the above text, it is your job to \\\n",
    "create a quiz  of {number} multiple choice questions for {subject} students in {tone} tone. \n",
    "Make sure the questions are not repeated and check all the questions to be conforming the text as well.\n",
    "Make sure to format your response like  RESPONSE_JSON below  and use it as a guide. \\\n",
    "Ensure to make {number} MCQs\n",
    "### RESPONSE_JSON\n",
    "{response_json}\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 639,
   "metadata": {},
   "outputs": [],
   "source": [
    "quiz_generation_prompt = PromptTemplate(\n",
    "    input_variables=[\"text\", \"number\", \"subject\", \"tone\", \"response_json\"],\n",
    "    template=TEMPLATE\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 640,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "quiz_chain=LLMChain(llm=llm, prompt=quiz_generation_prompt, output_key=\"quiz\", verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 641,
   "metadata": {},
   "outputs": [],
   "source": [
    "TEMPLATE2=\"\"\"\n",
    "You are an expert english grammarian and writer. Given a Multiple Choice Quiz for {subject} students.\\\n",
    "You need to evaluate the complexity of the question and give a complete analysis of the quiz. Only use at max 50 words for complexity analysis. \n",
    "if the quiz is not at per with the cognitive and analytical abilities of the students,\\\n",
    "update the quiz questions which needs to be changed and change the tone such that it perfectly fits the student abilities\n",
    "Quiz_MCQs:\n",
    "{quiz}\n",
    "\n",
    "Check from an expert English Writer of the above quiz:\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 642,
   "metadata": {},
   "outputs": [],
   "source": [
    "quiz_evaluation_prompt=PromptTemplate(input_variables=[\"subject\", \"quiz\"], template=TEMPLATE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 643,
   "metadata": {},
   "outputs": [],
   "source": [
    "review_chain=LLMChain(llm=llm, prompt=quiz_evaluation_prompt, output_key=\"review\", verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 644,
   "metadata": {},
   "outputs": [],
   "source": [
    "generate_evaluate_chain=SequentialChain(chains=[quiz_chain, review_chain], input_variables=[\"text\", \"number\", \"subject\", \"tone\", \"response_json\"],\n",
    "                                        output_variables=[\"quiz\", \"review\"], verbose=True,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 645,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path=r\"C:\\Users\\acer\\AdamGen\\data.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 646,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'C:\\\\Users\\\\acer\\\\AdamGen\\\\data.txt'"
      ]
     },
     "execution_count": 646,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "file_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 647,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(file_path, 'r') as file:\n",
    "    TEXT = file.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 648,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Polynomial Regression\n",
      "2.3.1 Polynomial Regression Intuition\n",
      "Is Polynomial Regression a linear or non linear model?\n",
      "That depends on what you are referring to. Polynomial Regression is linear on the coefficients since we don’t\n",
      "have any power of the coefficients (all the coefficients are raised to the power of 1: b0, b1, ..., bn). However,\n",
      "Polynomial Regression is a non linear function of the input x, since we have the inputs raised to several\n",
      "powers: x (power 1), x\n",
      "2\n",
      "(power 2), ..., x\n",
      "n (power n). That is how we can also see the Polynomial Regression\n",
      "as a non linear model. Besides indeed, Polynomial Regression is appropriate when the data is non linearly\n",
      "distributed (meaning you can’t fit a straight line between y and x).\n",
      "Page 8 of 51\n",
      "Machine Learning A-Z Q&A\n",
      "2.3.2 Polynomial Regression in Python\n",
      "Why didn’t we apply Feature Scaling in our Polynomial Regression model?\n",
      "It’s simply because, since y is a linear combination of x and x\n",
      "2\n",
      ", the coefficients can adapt their scale to put\n",
      "everything on the same scale. For example if y takes values between 0 and 1, x takes values between 1 and\n",
      "10 and x\n",
      "2\n",
      "takes values between 1 and 100, then b1 can be multiplied by 0.1 and b2 can be multiplied by 0.01\n",
      "so that y, b1x1 and b2x2 are all on the same scale.\n",
      "How do we find the best degree?\n",
      "The main form of finding a good fit is to plot the model and see what it looks like visually. You simply\n",
      "test several degrees and you see which one gives you the best fit. The other option is to find the lowest\n",
      "root-mean-square error (RMSE) for your model, but in that case be careful not to overfit the data.\n",
      "Why did we have to create a second linear regressor ’lin_reg_2’ ? Could we not have used\n",
      "’lin_reg’ directly?\n",
      "No, because ’lin_reg’ is already fitted to X and y and now we want to fit a new linear model to Xpoly and\n",
      "y. So we have to create a new regressor object. One must important that the fit method here finds the\n",
      "coefficient between the independent variables and the dependent variable. Therefore since ’lin_reg’ already\n",
      "got the coefficients of correlation between X and y, ’lin_reg_2’ has to be created to get some new coefficients\n",
      "of correlations between Xpoly and y.\n",
      "2.3.3 Polynomial Regression in R\n",
      "In R, I have to create manually the different columns for each polynomial feature? What if\n",
      "there are many polynomial features?\n",
      "You will very rarely have to create more than 4 polynomial features, otherwise you would get overfitting,\n",
      "which must absolutely be avoided in Machine Learning. So even if you have to create them manually, it will\n",
      "never take too much of your time. Besides you can use the template.\n",
      "How do we find the best degree? (Asking this question again in case some students only do R\n",
      "The main form of finding a good fit is to plot the model and see what it looks like visually. You simply\n",
      "test several degrees and you see which one gives you the best fit. The other option is to find the lowest\n",
      "root-mean-square error (RMSE) for your model, but in that case be careful not to overfit the data.\n",
      "2.4 SVR\n",
      "2.4.1 SVR Intuition\n",
      "When should I use SVR?\n",
      "You should use SVR if a linear model like linear regression doesn’t fit very well your data. This would mean\n",
      "you are dealing with a non linear problem, where your data is not linearly distributed. Therefore in that\n",
      "case SVR could be a much better solution.\n",
      "I didn’t understand the Intuition Lecture. Am I in trouble?\n",
      "Not at all. SVR is a pretty abstract model and besides it is not that commonly used. What you must rather\n",
      "understand is the SVM model, which you will see in Part 3 - Classification. Then once you understand the\n",
      "SVM model, you will get a better grasp of the SVR model, since the SVR is simply the SVM for Regression.\n",
      "However we wanted to include SVR in this course to give you an extra option in your Machine Learning\n",
      "toolkit.\n",
      "Page 9 of 51\n",
      "Machine Learning A-Z Q&A\n",
      "2.4.2 SVR in Python\n",
      "Why do we need to ’sc_Y.inverse_transform’ ?\n",
      "We need the inverse_transform method to go back to the original scale. Indeed we applied feature scaling\n",
      "so we get this scale around 0 and if we make a prediction without inversing the scale we will get the\n",
      "scaled predicted salary. And of course we want the real salary, not the scaled one, so we have to use\n",
      "’sc_Y.inverse_transform’. Also what is important to understand is that ’transform’ and ’inverse_transform’\n",
      "are paired methods.\n",
      "2.4.3 SVR in R\n",
      "Why did we not apply feature scaling like we did explicitly in Python\n",
      "That’s because in svm() function of R, the values are automatically scaled.\n",
      "Can we select the most significant variables thanks to the p-value like we did in R before?\n",
      "You couldn’t use p-value because SVR is not a linear model, and p-values apply only to linear models.\n",
      "Therefore feature selection is out of the question. But you could do feature extraction, which you will see in\n",
      "Part 9 - Dimensionality Reduction. That you can apply to Decision Trees, and it will reduce the number of\n",
      "your features.\n",
      "2.5 Decision Tree Regression\n",
      "2.5.1 Decision Tree Regression Intuition\n",
      "How does the algorithm split the data points?\n",
      "It uses reduction of standard deviation of the predictions. In other words, the standard deviation is decreased\n",
      "right after a split. Hence, building a decision tree is all about finding the attribute that returns the highest\n",
      "standard deviation reduction (i.e., the most homogeneous branches).\n",
      "What is the Information Gain and how does it work in Decision Trees?\n",
      "The Information Gain in Decision Tree Regression is exactly the Standard Deviation Reduction we are\n",
      "looking to reach. We calculate by how much the Standard Deviation decreases after each split. Because the\n",
      "more the Standard Deviation is decreased after a split, the more homogeneous the child nodes will be.\n",
      "What is the Entropy and how does it work in Decision Trees?\n",
      "The Entropy measures the disorder in a set, here in a part resulting from a split. So the more homogeneous\n",
      "is your data in a part, the lower will be the entropy. The more you have splits, the more you have chance\n",
      "to find parts in which your data is homogeneous, and therefore the lower will be the entropy (close to 0) in\n",
      "these parts. However you might still find some nodes where the data is not homogeneous, and therefore the\n",
      "entropy would not be that small.\n",
      "2.5.2 Decision Tree Regression in Python\n",
      "Does a Decision Tree make much sense in 1D?\n",
      "Not really, as we saw in the practical part of this section. In 1D (meaning one independent variable), the\n",
      "Decision Tree clearly tends to overfit the data. The Decision Tree would be much more relevant in higher\n",
      "dimension, but keep in mind that the implementation we made here in 1D would be exactly the same in\n",
      "higher dimension. Therefore you might want to keep that model in your toolkit in case you are dealing\n",
      "with a higher dimensional space. This will actually be the case in Part 3 - Classification, where we will use\n",
      "Decision Tree for Classification in 2D, which you will see turns out to be more relevant.\n",
      "Page 10 of 51\n",
      "Machine Learning A-Z Q&A\n",
      "2.5.3 Decision Tree Regression in R\n",
      "Why do we get different results between Python and R?\n",
      "The difference is likely due to the random split of data. If we did a cross-validation (see Part 10) on all the\n",
      "models in both languages, then you would likely get a similar mean accuracy. That being said, we would\n",
      "recommend more using Python for Decision Trees since the model is slightly better implemented in Python.\n",
      "Is the Decision Tree appropriate here?\n",
      "Here in this example, we can clearly see that the fitting curve is a stair with large gaps in the discontinuities.\n",
      "That decision tree regression model is therefore not the most appropriate, and that is because we have only\n",
      "one independent variables taking discrete values. So what happened is that the prediction was made in the\n",
      "lower part of the gap in Python, and made in the upper part of the gap in R. And since the gap is large, that\n",
      "makes a big difference. If we had much more observations, taking values with more continuity (like with a\n",
      "0.1 step), the gaps would be smaller and therefore the predictions in Python and R far from each other.\n",
      "Can we select the most significant variables thanks to the p-value like we did in R before?\n",
      "You couldn’t use p-value because Decision Tree is not a linear model, and p-values apply only to linear\n",
      "models. Therefore feature selection is out of the question. But you could do feature extraction, which you\n",
      "will see in Part 9 - Dimensionality Reduction. That you can apply to Decision Trees, and it will reduce the\n",
      "number of your features.\n",
      "2.6 Random Forest Regression\n",
      "2.6.1 Random Forest Regression Intuition\n",
      "What is the advantage and drawback of Random Forests compared to Decision Trees? Advan\u0002tage: Random Forests can give you a better predictive power than Decision Trees.\n",
      "Drawback: Decision Tree will give you more interpretability than Random Forests, because you can plot the\n",
      "graph of a Decision Tree to see the different splits leading to the prediction, as seen in the Intuition Lecture.\n",
      "That’s something you can’t do with Random Forests.\n",
      "When to use Random Forest and when to use the other models?\n",
      "The best answer to that question is: try them all!\n",
      "Indeed, thanks to the templates it will only take you 10 minutes to try all the models, which is very little\n",
      "compared to the time dedicated to the other parts of a data science project (like Data Preprocessing for\n",
      "example). So just don’t be scared to try all the regression models and compare the results (through cross\n",
      "validation which we will see in Part \n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(TEXT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 649,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'{\"1\": {\"mcq\": \"multiple choice question\", \"options\": {\"a\": \"choice here\", \"b\": \"choice here\", \"c\": \"choice here\", \"d\": \"choice here\"}, \"correct\": \"correct answer\"}, \"2\": {\"mcq\": \"multiple choice question\", \"options\": {\"a\": \"choice here\", \"b\": \"choice here\", \"c\": \"choice here\", \"d\": \"choice here\"}, \"correct\": \"correct answer\"}, \"3\": {\"mcq\": \"multiple choice question\", \"options\": {\"a\": \"choice here\", \"b\": \"choice here\", \"c\": \"choice here\", \"d\": \"choice here\"}, \"correct\": \"correct answer\"}}'"
      ]
     },
     "execution_count": 649,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Serialize the Python dictionary into a JSON-formatted string\n",
    "json.dumps(RESPONSE_JSON)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 650,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUMBER=8\n",
    "SUBJECT=\"Machine Learning\"\n",
    "TONE=\"simple\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 651,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new SequentialChain chain...\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "Text:Polynomial Regression\n",
      "2.3.1 Polynomial Regression Intuition\n",
      "Is Polynomial Regression a linear or non linear model?\n",
      "That depends on what you are referring to. Polynomial Regression is linear on the coefficients since we don’t\n",
      "have any power of the coefficients (all the coefficients are raised to the power of 1: b0, b1, ..., bn). However,\n",
      "Polynomial Regression is a non linear function of the input x, since we have the inputs raised to several\n",
      "powers: x (power 1), x\n",
      "2\n",
      "(power 2), ..., x\n",
      "n (power n). That is how we can also see the Polynomial Regression\n",
      "as a non linear model. Besides indeed, Polynomial Regression is appropriate when the data is non linearly\n",
      "distributed (meaning you can’t fit a straight line between y and x).\n",
      "Page 8 of 51\n",
      "Machine Learning A-Z Q&A\n",
      "2.3.2 Polynomial Regression in Python\n",
      "Why didn’t we apply Feature Scaling in our Polynomial Regression model?\n",
      "It’s simply because, since y is a linear combination of x and x\n",
      "2\n",
      ", the coefficients can adapt their scale to put\n",
      "everything on the same scale. For example if y takes values between 0 and 1, x takes values between 1 and\n",
      "10 and x\n",
      "2\n",
      "takes values between 1 and 100, then b1 can be multiplied by 0.1 and b2 can be multiplied by 0.01\n",
      "so that y, b1x1 and b2x2 are all on the same scale.\n",
      "How do we find the best degree?\n",
      "The main form of finding a good fit is to plot the model and see what it looks like visually. You simply\n",
      "test several degrees and you see which one gives you the best fit. The other option is to find the lowest\n",
      "root-mean-square error (RMSE) for your model, but in that case be careful not to overfit the data.\n",
      "Why did we have to create a second linear regressor ’lin_reg_2’ ? Could we not have used\n",
      "’lin_reg’ directly?\n",
      "No, because ’lin_reg’ is already fitted to X and y and now we want to fit a new linear model to Xpoly and\n",
      "y. So we have to create a new regressor object. One must important that the fit method here finds the\n",
      "coefficient between the independent variables and the dependent variable. Therefore since ’lin_reg’ already\n",
      "got the coefficients of correlation between X and y, ’lin_reg_2’ has to be created to get some new coefficients\n",
      "of correlations between Xpoly and y.\n",
      "2.3.3 Polynomial Regression in R\n",
      "In R, I have to create manually the different columns for each polynomial feature? What if\n",
      "there are many polynomial features?\n",
      "You will very rarely have to create more than 4 polynomial features, otherwise you would get overfitting,\n",
      "which must absolutely be avoided in Machine Learning. So even if you have to create them manually, it will\n",
      "never take too much of your time. Besides you can use the template.\n",
      "How do we find the best degree? (Asking this question again in case some students only do R\n",
      "The main form of finding a good fit is to plot the model and see what it looks like visually. You simply\n",
      "test several degrees and you see which one gives you the best fit. The other option is to find the lowest\n",
      "root-mean-square error (RMSE) for your model, but in that case be careful not to overfit the data.\n",
      "2.4 SVR\n",
      "2.4.1 SVR Intuition\n",
      "When should I use SVR?\n",
      "You should use SVR if a linear model like linear regression doesn’t fit very well your data. This would mean\n",
      "you are dealing with a non linear problem, where your data is not linearly distributed. Therefore in that\n",
      "case SVR could be a much better solution.\n",
      "I didn’t understand the Intuition Lecture. Am I in trouble?\n",
      "Not at all. SVR is a pretty abstract model and besides it is not that commonly used. What you must rather\n",
      "understand is the SVM model, which you will see in Part 3 - Classification. Then once you understand the\n",
      "SVM model, you will get a better grasp of the SVR model, since the SVR is simply the SVM for Regression.\n",
      "However we wanted to include SVR in this course to give you an extra option in your Machine Learning\n",
      "toolkit.\n",
      "Page 9 of 51\n",
      "Machine Learning A-Z Q&A\n",
      "2.4.2 SVR in Python\n",
      "Why do we need to ’sc_Y.inverse_transform’ ?\n",
      "We need the inverse_transform method to go back to the original scale. Indeed we applied feature scaling\n",
      "so we get this scale around 0 and if we make a prediction without inversing the scale we will get the\n",
      "scaled predicted salary. And of course we want the real salary, not the scaled one, so we have to use\n",
      "’sc_Y.inverse_transform’. Also what is important to understand is that ’transform’ and ’inverse_transform’\n",
      "are paired methods.\n",
      "2.4.3 SVR in R\n",
      "Why did we not apply feature scaling like we did explicitly in Python\n",
      "That’s because in svm() function of R, the values are automatically scaled.\n",
      "Can we select the most significant variables thanks to the p-value like we did in R before?\n",
      "You couldn’t use p-value because SVR is not a linear model, and p-values apply only to linear models.\n",
      "Therefore feature selection is out of the question. But you could do feature extraction, which you will see in\n",
      "Part 9 - Dimensionality Reduction. That you can apply to Decision Trees, and it will reduce the number of\n",
      "your features.\n",
      "2.5 Decision Tree Regression\n",
      "2.5.1 Decision Tree Regression Intuition\n",
      "How does the algorithm split the data points?\n",
      "It uses reduction of standard deviation of the predictions. In other words, the standard deviation is decreased\n",
      "right after a split. Hence, building a decision tree is all about finding the attribute that returns the highest\n",
      "standard deviation reduction (i.e., the most homogeneous branches).\n",
      "What is the Information Gain and how does it work in Decision Trees?\n",
      "The Information Gain in Decision Tree Regression is exactly the Standard Deviation Reduction we are\n",
      "looking to reach. We calculate by how much the Standard Deviation decreases after each split. Because the\n",
      "more the Standard Deviation is decreased after a split, the more homogeneous the child nodes will be.\n",
      "What is the Entropy and how does it work in Decision Trees?\n",
      "The Entropy measures the disorder in a set, here in a part resulting from a split. So the more homogeneous\n",
      "is your data in a part, the lower will be the entropy. The more you have splits, the more you have chance\n",
      "to find parts in which your data is homogeneous, and therefore the lower will be the entropy (close to 0) in\n",
      "these parts. However you might still find some nodes where the data is not homogeneous, and therefore the\n",
      "entropy would not be that small.\n",
      "2.5.2 Decision Tree Regression in Python\n",
      "Does a Decision Tree make much sense in 1D?\n",
      "Not really, as we saw in the practical part of this section. In 1D (meaning one independent variable), the\n",
      "Decision Tree clearly tends to overfit the data. The Decision Tree would be much more relevant in higher\n",
      "dimension, but keep in mind that the implementation we made here in 1D would be exactly the same in\n",
      "higher dimension. Therefore you might want to keep that model in your toolkit in case you are dealing\n",
      "with a higher dimensional space. This will actually be the case in Part 3 - Classification, where we will use\n",
      "Decision Tree for Classification in 2D, which you will see turns out to be more relevant.\n",
      "Page 10 of 51\n",
      "Machine Learning A-Z Q&A\n",
      "2.5.3 Decision Tree Regression in R\n",
      "Why do we get different results between Python and R?\n",
      "The difference is likely due to the random split of data. If we did a cross-validation (see Part 10) on all the\n",
      "models in both languages, then you would likely get a similar mean accuracy. That being said, we would\n",
      "recommend more using Python for Decision Trees since the model is slightly better implemented in Python.\n",
      "Is the Decision Tree appropriate here?\n",
      "Here in this example, we can clearly see that the fitting curve is a stair with large gaps in the discontinuities.\n",
      "That decision tree regression model is therefore not the most appropriate, and that is because we have only\n",
      "one independent variables taking discrete values. So what happened is that the prediction was made in the\n",
      "lower part of the gap in Python, and made in the upper part of the gap in R. And since the gap is large, that\n",
      "makes a big difference. If we had much more observations, taking values with more continuity (like with a\n",
      "0.1 step), the gaps would be smaller and therefore the predictions in Python and R far from each other.\n",
      "Can we select the most significant variables thanks to the p-value like we did in R before?\n",
      "You couldn’t use p-value because Decision Tree is not a linear model, and p-values apply only to linear\n",
      "models. Therefore feature selection is out of the question. But you could do feature extraction, which you\n",
      "will see in Part 9 - Dimensionality Reduction. That you can apply to Decision Trees, and it will reduce the\n",
      "number of your features.\n",
      "2.6 Random Forest Regression\n",
      "2.6.1 Random Forest Regression Intuition\n",
      "What is the advantage and drawback of Random Forests compared to Decision Trees? Advan\u0002tage: Random Forests can give you a better predictive power than Decision Trees.\n",
      "Drawback: Decision Tree will give you more interpretability than Random Forests, because you can plot the\n",
      "graph of a Decision Tree to see the different splits leading to the prediction, as seen in the Intuition Lecture.\n",
      "That’s something you can’t do with Random Forests.\n",
      "When to use Random Forest and when to use the other models?\n",
      "The best answer to that question is: try them all!\n",
      "Indeed, thanks to the templates it will only take you 10 minutes to try all the models, which is very little\n",
      "compared to the time dedicated to the other parts of a data science project (like Data Preprocessing for\n",
      "example). So just don’t be scared to try all the regression models and compare the results (through cross\n",
      "validation which we will see in Part \n",
      "\n",
      "You are an expert MCQ maker. Given the above text, it is your job to create a quiz  of 8 multiple choice questions for Machine Learning students in simple tone. \n",
      "Make sure the questions are not repeated and check all the questions to be conforming the text as well.\n",
      "Make sure to format your response like  RESPONSE_JSON below  and use it as a guide. Ensure to make 8 MCQs\n",
      "### RESPONSE_JSON\n",
      "{\"1\": {\"mcq\": \"multiple choice question\", \"options\": {\"a\": \"choice here\", \"b\": \"choice here\", \"c\": \"choice here\", \"d\": \"choice here\"}, \"correct\": \"correct answer\"}, \"2\": {\"mcq\": \"multiple choice question\", \"options\": {\"a\": \"choice here\", \"b\": \"choice here\", \"c\": \"choice here\", \"d\": \"choice here\"}, \"correct\": \"correct answer\"}, \"3\": {\"mcq\": \"multiple choice question\", \"options\": {\"a\": \"choice here\", \"b\": \"choice here\", \"c\": \"choice here\", \"d\": \"choice here\"}, \"correct\": \"correct answer\"}}\n",
      "\n",
      "\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\n",
      "\u001b[1m> Entering new LLMChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3m\n",
      "Text:Polynomial Regression\n",
      "2.3.1 Polynomial Regression Intuition\n",
      "Is Polynomial Regression a linear or non linear model?\n",
      "That depends on what you are referring to. Polynomial Regression is linear on the coefficients since we don’t\n",
      "have any power of the coefficients (all the coefficients are raised to the power of 1: b0, b1, ..., bn). However,\n",
      "Polynomial Regression is a non linear function of the input x, since we have the inputs raised to several\n",
      "powers: x (power 1), x\n",
      "2\n",
      "(power 2), ..., x\n",
      "n (power n). That is how we can also see the Polynomial Regression\n",
      "as a non linear model. Besides indeed, Polynomial Regression is appropriate when the data is non linearly\n",
      "distributed (meaning you can’t fit a straight line between y and x).\n",
      "Page 8 of 51\n",
      "Machine Learning A-Z Q&A\n",
      "2.3.2 Polynomial Regression in Python\n",
      "Why didn’t we apply Feature Scaling in our Polynomial Regression model?\n",
      "It’s simply because, since y is a linear combination of x and x\n",
      "2\n",
      ", the coefficients can adapt their scale to put\n",
      "everything on the same scale. For example if y takes values between 0 and 1, x takes values between 1 and\n",
      "10 and x\n",
      "2\n",
      "takes values between 1 and 100, then b1 can be multiplied by 0.1 and b2 can be multiplied by 0.01\n",
      "so that y, b1x1 and b2x2 are all on the same scale.\n",
      "How do we find the best degree?\n",
      "The main form of finding a good fit is to plot the model and see what it looks like visually. You simply\n",
      "test several degrees and you see which one gives you the best fit. The other option is to find the lowest\n",
      "root-mean-square error (RMSE) for your model, but in that case be careful not to overfit the data.\n",
      "Why did we have to create a second linear regressor ’lin_reg_2’ ? Could we not have used\n",
      "’lin_reg’ directly?\n",
      "No, because ’lin_reg’ is already fitted to X and y and now we want to fit a new linear model to Xpoly and\n",
      "y. So we have to create a new regressor object. One must important that the fit method here finds the\n",
      "coefficient between the independent variables and the dependent variable. Therefore since ’lin_reg’ already\n",
      "got the coefficients of correlation between X and y, ’lin_reg_2’ has to be created to get some new coefficients\n",
      "of correlations between Xpoly and y.\n",
      "2.3.3 Polynomial Regression in R\n",
      "In R, I have to create manually the different columns for each polynomial feature? What if\n",
      "there are many polynomial features?\n",
      "You will very rarely have to create more than 4 polynomial features, otherwise you would get overfitting,\n",
      "which must absolutely be avoided in Machine Learning. So even if you have to create them manually, it will\n",
      "never take too much of your time. Besides you can use the template.\n",
      "How do we find the best degree? (Asking this question again in case some students only do R\n",
      "The main form of finding a good fit is to plot the model and see what it looks like visually. You simply\n",
      "test several degrees and you see which one gives you the best fit. The other option is to find the lowest\n",
      "root-mean-square error (RMSE) for your model, but in that case be careful not to overfit the data.\n",
      "2.4 SVR\n",
      "2.4.1 SVR Intuition\n",
      "When should I use SVR?\n",
      "You should use SVR if a linear model like linear regression doesn’t fit very well your data. This would mean\n",
      "you are dealing with a non linear problem, where your data is not linearly distributed. Therefore in that\n",
      "case SVR could be a much better solution.\n",
      "I didn’t understand the Intuition Lecture. Am I in trouble?\n",
      "Not at all. SVR is a pretty abstract model and besides it is not that commonly used. What you must rather\n",
      "understand is the SVM model, which you will see in Part 3 - Classification. Then once you understand the\n",
      "SVM model, you will get a better grasp of the SVR model, since the SVR is simply the SVM for Regression.\n",
      "However we wanted to include SVR in this course to give you an extra option in your Machine Learning\n",
      "toolkit.\n",
      "Page 9 of 51\n",
      "Machine Learning A-Z Q&A\n",
      "2.4.2 SVR in Python\n",
      "Why do we need to ’sc_Y.inverse_transform’ ?\n",
      "We need the inverse_transform method to go back to the original scale. Indeed we applied feature scaling\n",
      "so we get this scale around 0 and if we make a prediction without inversing the scale we will get the\n",
      "scaled predicted salary. And of course we want the real salary, not the scaled one, so we have to use\n",
      "’sc_Y.inverse_transform’. Also what is important to understand is that ’transform’ and ’inverse_transform’\n",
      "are paired methods.\n",
      "2.4.3 SVR in R\n",
      "Why did we not apply feature scaling like we did explicitly in Python\n",
      "That’s because in svm() function of R, the values are automatically scaled.\n",
      "Can we select the most significant variables thanks to the p-value like we did in R before?\n",
      "You couldn’t use p-value because SVR is not a linear model, and p-values apply only to linear models.\n",
      "Therefore feature selection is out of the question. But you could do feature extraction, which you will see in\n",
      "Part 9 - Dimensionality Reduction. That you can apply to Decision Trees, and it will reduce the number of\n",
      "your features.\n",
      "2.5 Decision Tree Regression\n",
      "2.5.1 Decision Tree Regression Intuition\n",
      "How does the algorithm split the data points?\n",
      "It uses reduction of standard deviation of the predictions. In other words, the standard deviation is decreased\n",
      "right after a split. Hence, building a decision tree is all about finding the attribute that returns the highest\n",
      "standard deviation reduction (i.e., the most homogeneous branches).\n",
      "What is the Information Gain and how does it work in Decision Trees?\n",
      "The Information Gain in Decision Tree Regression is exactly the Standard Deviation Reduction we are\n",
      "looking to reach. We calculate by how much the Standard Deviation decreases after each split. Because the\n",
      "more the Standard Deviation is decreased after a split, the more homogeneous the child nodes will be.\n",
      "What is the Entropy and how does it work in Decision Trees?\n",
      "The Entropy measures the disorder in a set, here in a part resulting from a split. So the more homogeneous\n",
      "is your data in a part, the lower will be the entropy. The more you have splits, the more you have chance\n",
      "to find parts in which your data is homogeneous, and therefore the lower will be the entropy (close to 0) in\n",
      "these parts. However you might still find some nodes where the data is not homogeneous, and therefore the\n",
      "entropy would not be that small.\n",
      "2.5.2 Decision Tree Regression in Python\n",
      "Does a Decision Tree make much sense in 1D?\n",
      "Not really, as we saw in the practical part of this section. In 1D (meaning one independent variable), the\n",
      "Decision Tree clearly tends to overfit the data. The Decision Tree would be much more relevant in higher\n",
      "dimension, but keep in mind that the implementation we made here in 1D would be exactly the same in\n",
      "higher dimension. Therefore you might want to keep that model in your toolkit in case you are dealing\n",
      "with a higher dimensional space. This will actually be the case in Part 3 - Classification, where we will use\n",
      "Decision Tree for Classification in 2D, which you will see turns out to be more relevant.\n",
      "Page 10 of 51\n",
      "Machine Learning A-Z Q&A\n",
      "2.5.3 Decision Tree Regression in R\n",
      "Why do we get different results between Python and R?\n",
      "The difference is likely due to the random split of data. If we did a cross-validation (see Part 10) on all the\n",
      "models in both languages, then you would likely get a similar mean accuracy. That being said, we would\n",
      "recommend more using Python for Decision Trees since the model is slightly better implemented in Python.\n",
      "Is the Decision Tree appropriate here?\n",
      "Here in this example, we can clearly see that the fitting curve is a stair with large gaps in the discontinuities.\n",
      "That decision tree regression model is therefore not the most appropriate, and that is because we have only\n",
      "one independent variables taking discrete values. So what happened is that the prediction was made in the\n",
      "lower part of the gap in Python, and made in the upper part of the gap in R. And since the gap is large, that\n",
      "makes a big difference. If we had much more observations, taking values with more continuity (like with a\n",
      "0.1 step), the gaps would be smaller and therefore the predictions in Python and R far from each other.\n",
      "Can we select the most significant variables thanks to the p-value like we did in R before?\n",
      "You couldn’t use p-value because Decision Tree is not a linear model, and p-values apply only to linear\n",
      "models. Therefore feature selection is out of the question. But you could do feature extraction, which you\n",
      "will see in Part 9 - Dimensionality Reduction. That you can apply to Decision Trees, and it will reduce the\n",
      "number of your features.\n",
      "2.6 Random Forest Regression\n",
      "2.6.1 Random Forest Regression Intuition\n",
      "What is the advantage and drawback of Random Forests compared to Decision Trees? Advan\u0002tage: Random Forests can give you a better predictive power than Decision Trees.\n",
      "Drawback: Decision Tree will give you more interpretability than Random Forests, because you can plot the\n",
      "graph of a Decision Tree to see the different splits leading to the prediction, as seen in the Intuition Lecture.\n",
      "That’s something you can’t do with Random Forests.\n",
      "When to use Random Forest and when to use the other models?\n",
      "The best answer to that question is: try them all!\n",
      "Indeed, thanks to the templates it will only take you 10 minutes to try all the models, which is very little\n",
      "compared to the time dedicated to the other parts of a data science project (like Data Preprocessing for\n",
      "example). So just don’t be scared to try all the regression models and compare the results (through cross\n",
      "validation which we will see in Part \n",
      "\n",
      "You are an expert MCQ maker. Given the above text, it is your job to create a quiz  of 8 multiple choice questions for Machine Learning students in simple tone. \n",
      "Make sure the questions are not repeated and check all the questions to be conforming the text as well.\n",
      "Make sure to format your response like  RESPONSE_JSON below  and use it as a guide. Ensure to make 8 MCQs\n",
      "### RESPONSE_JSON\n",
      "{\"1\": {\"mcq\": \"multiple choice question\", \"options\": {\"a\": \"choice here\", \"b\": \"choice here\", \"c\": \"choice here\", \"d\": \"choice here\"}, \"correct\": \"correct answer\"}, \"2\": {\"mcq\": \"multiple choice question\", \"options\": {\"a\": \"choice here\", \"b\": \"choice here\", \"c\": \"choice here\", \"d\": \"choice here\"}, \"correct\": \"correct answer\"}, \"3\": {\"mcq\": \"multiple choice question\", \"options\": {\"a\": \"choice here\", \"b\": \"choice here\", \"c\": \"choice here\", \"d\": \"choice here\"}, \"correct\": \"correct answer\"}}\n",
      "\n",
      "\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "#https://python.langchain.com/docs/modules/model_io/llms/token_usage_tracking\n",
    "\n",
    "#How to setup Token Usage Tracking in LangChain\n",
    "with get_openai_callback() as cb:\n",
    "    response=generate_evaluate_chain(\n",
    "        {\n",
    "            \"text\": TEXT,\n",
    "            \"number\": NUMBER,\n",
    "            \"subject\":SUBJECT,\n",
    "            \"tone\": TONE,\n",
    "            \"response_json\": json.dumps(RESPONSE_JSON)\n",
    "        }\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 652,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Tokens:6504\n",
      "Prompt Tokens:4820\n",
      "Completion Tokens:1684\n",
      "Total Cost:0.010598\n"
     ]
    }
   ],
   "source": [
    "print(f\"Total Tokens:{cb.total_tokens}\")\n",
    "print(f\"Prompt Tokens:{cb.prompt_tokens}\")\n",
    "print(f\"Completion Tokens:{cb.completion_tokens}\")\n",
    "print(f\"Total Cost:{cb.total_cost}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 653,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'text': 'Polynomial Regression\\n2.3.1 Polynomial Regression Intuition\\nIs Polynomial Regression a linear or non linear model?\\nThat depends on what you are referring to. Polynomial Regression is linear on the coefficients since we don’t\\nhave any power of the coefficients (all the coefficients are raised to the power of 1: b0, b1, ..., bn). However,\\nPolynomial Regression is a non linear function of the input x, since we have the inputs raised to several\\npowers: x (power 1), x\\n2\\n(power 2), ..., x\\nn (power n). That is how we can also see the Polynomial Regression\\nas a non linear model. Besides indeed, Polynomial Regression is appropriate when the data is non linearly\\ndistributed (meaning you can’t fit a straight line between y and x).\\nPage 8 of 51\\nMachine Learning A-Z Q&A\\n2.3.2 Polynomial Regression in Python\\nWhy didn’t we apply Feature Scaling in our Polynomial Regression model?\\nIt’s simply because, since y is a linear combination of x and x\\n2\\n, the coefficients can adapt their scale to put\\neverything on the same scale. For example if y takes values between 0 and 1, x takes values between 1 and\\n10 and x\\n2\\ntakes values between 1 and 100, then b1 can be multiplied by 0.1 and b2 can be multiplied by 0.01\\nso that y, b1x1 and b2x2 are all on the same scale.\\nHow do we find the best degree?\\nThe main form of finding a good fit is to plot the model and see what it looks like visually. You simply\\ntest several degrees and you see which one gives you the best fit. The other option is to find the lowest\\nroot-mean-square error (RMSE) for your model, but in that case be careful not to overfit the data.\\nWhy did we have to create a second linear regressor ’lin_reg_2’ ? Could we not have used\\n’lin_reg’ directly?\\nNo, because ’lin_reg’ is already fitted to X and y and now we want to fit a new linear model to Xpoly and\\ny. So we have to create a new regressor object. One must important that the fit method here finds the\\ncoefficient between the independent variables and the dependent variable. Therefore since ’lin_reg’ already\\ngot the coefficients of correlation between X and y, ’lin_reg_2’ has to be created to get some new coefficients\\nof correlations between Xpoly and y.\\n2.3.3 Polynomial Regression in R\\nIn R, I have to create manually the different columns for each polynomial feature? What if\\nthere are many polynomial features?\\nYou will very rarely have to create more than 4 polynomial features, otherwise you would get overfitting,\\nwhich must absolutely be avoided in Machine Learning. So even if you have to create them manually, it will\\nnever take too much of your time. Besides you can use the template.\\nHow do we find the best degree? (Asking this question again in case some students only do R\\nThe main form of finding a good fit is to plot the model and see what it looks like visually. You simply\\ntest several degrees and you see which one gives you the best fit. The other option is to find the lowest\\nroot-mean-square error (RMSE) for your model, but in that case be careful not to overfit the data.\\n2.4 SVR\\n2.4.1 SVR Intuition\\nWhen should I use SVR?\\nYou should use SVR if a linear model like linear regression doesn’t fit very well your data. This would mean\\nyou are dealing with a non linear problem, where your data is not linearly distributed. Therefore in that\\ncase SVR could be a much better solution.\\nI didn’t understand the Intuition Lecture. Am I in trouble?\\nNot at all. SVR is a pretty abstract model and besides it is not that commonly used. What you must rather\\nunderstand is the SVM model, which you will see in Part 3 - Classification. Then once you understand the\\nSVM model, you will get a better grasp of the SVR model, since the SVR is simply the SVM for Regression.\\nHowever we wanted to include SVR in this course to give you an extra option in your Machine Learning\\ntoolkit.\\nPage 9 of 51\\nMachine Learning A-Z Q&A\\n2.4.2 SVR in Python\\nWhy do we need to ’sc_Y.inverse_transform’ ?\\nWe need the inverse_transform method to go back to the original scale. Indeed we applied feature scaling\\nso we get this scale around 0 and if we make a prediction without inversing the scale we will get the\\nscaled predicted salary. And of course we want the real salary, not the scaled one, so we have to use\\n’sc_Y.inverse_transform’. Also what is important to understand is that ’transform’ and ’inverse_transform’\\nare paired methods.\\n2.4.3 SVR in R\\nWhy did we not apply feature scaling like we did explicitly in Python\\nThat’s because in svm() function of R, the values are automatically scaled.\\nCan we select the most significant variables thanks to the p-value like we did in R before?\\nYou couldn’t use p-value because SVR is not a linear model, and p-values apply only to linear models.\\nTherefore feature selection is out of the question. But you could do feature extraction, which you will see in\\nPart 9 - Dimensionality Reduction. That you can apply to Decision Trees, and it will reduce the number of\\nyour features.\\n2.5 Decision Tree Regression\\n2.5.1 Decision Tree Regression Intuition\\nHow does the algorithm split the data points?\\nIt uses reduction of standard deviation of the predictions. In other words, the standard deviation is decreased\\nright after a split. Hence, building a decision tree is all about finding the attribute that returns the highest\\nstandard deviation reduction (i.e., the most homogeneous branches).\\nWhat is the Information Gain and how does it work in Decision Trees?\\nThe Information Gain in Decision Tree Regression is exactly the Standard Deviation Reduction we are\\nlooking to reach. We calculate by how much the Standard Deviation decreases after each split. Because the\\nmore the Standard Deviation is decreased after a split, the more homogeneous the child nodes will be.\\nWhat is the Entropy and how does it work in Decision Trees?\\nThe Entropy measures the disorder in a set, here in a part resulting from a split. So the more homogeneous\\nis your data in a part, the lower will be the entropy. The more you have splits, the more you have chance\\nto find parts in which your data is homogeneous, and therefore the lower will be the entropy (close to 0) in\\nthese parts. However you might still find some nodes where the data is not homogeneous, and therefore the\\nentropy would not be that small.\\n2.5.2 Decision Tree Regression in Python\\nDoes a Decision Tree make much sense in 1D?\\nNot really, as we saw in the practical part of this section. In 1D (meaning one independent variable), the\\nDecision Tree clearly tends to overfit the data. The Decision Tree would be much more relevant in higher\\ndimension, but keep in mind that the implementation we made here in 1D would be exactly the same in\\nhigher dimension. Therefore you might want to keep that model in your toolkit in case you are dealing\\nwith a higher dimensional space. This will actually be the case in Part 3 - Classification, where we will use\\nDecision Tree for Classification in 2D, which you will see turns out to be more relevant.\\nPage 10 of 51\\nMachine Learning A-Z Q&A\\n2.5.3 Decision Tree Regression in R\\nWhy do we get different results between Python and R?\\nThe difference is likely due to the random split of data. If we did a cross-validation (see Part 10) on all the\\nmodels in both languages, then you would likely get a similar mean accuracy. That being said, we would\\nrecommend more using Python for Decision Trees since the model is slightly better implemented in Python.\\nIs the Decision Tree appropriate here?\\nHere in this example, we can clearly see that the fitting curve is a stair with large gaps in the discontinuities.\\nThat decision tree regression model is therefore not the most appropriate, and that is because we have only\\none independent variables taking discrete values. So what happened is that the prediction was made in the\\nlower part of the gap in Python, and made in the upper part of the gap in R. And since the gap is large, that\\nmakes a big difference. If we had much more observations, taking values with more continuity (like with a\\n0.1 step), the gaps would be smaller and therefore the predictions in Python and R far from each other.\\nCan we select the most significant variables thanks to the p-value like we did in R before?\\nYou couldn’t use p-value because Decision Tree is not a linear model, and p-values apply only to linear\\nmodels. Therefore feature selection is out of the question. But you could do feature extraction, which you\\nwill see in Part 9 - Dimensionality Reduction. That you can apply to Decision Trees, and it will reduce the\\nnumber of your features.\\n2.6 Random Forest Regression\\n2.6.1 Random Forest Regression Intuition\\nWhat is the advantage and drawback of Random Forests compared to Decision Trees? Advan\\x02tage: Random Forests can give you a better predictive power than Decision Trees.\\nDrawback: Decision Tree will give you more interpretability than Random Forests, because you can plot the\\ngraph of a Decision Tree to see the different splits leading to the prediction, as seen in the Intuition Lecture.\\nThat’s something you can’t do with Random Forests.\\nWhen to use Random Forest and when to use the other models?\\nThe best answer to that question is: try them all!\\nIndeed, thanks to the templates it will only take you 10 minutes to try all the models, which is very little\\ncompared to the time dedicated to the other parts of a data science project (like Data Preprocessing for\\nexample). So just don’t be scared to try all the regression models and compare the results (through cross\\nvalidation which we will see in Part \\n',\n",
       " 'number': 8,\n",
       " 'subject': 'Machine Learning',\n",
       " 'tone': 'simple',\n",
       " 'response_json': '{\"1\": {\"mcq\": \"multiple choice question\", \"options\": {\"a\": \"choice here\", \"b\": \"choice here\", \"c\": \"choice here\", \"d\": \"choice here\"}, \"correct\": \"correct answer\"}, \"2\": {\"mcq\": \"multiple choice question\", \"options\": {\"a\": \"choice here\", \"b\": \"choice here\", \"c\": \"choice here\", \"d\": \"choice here\"}, \"correct\": \"correct answer\"}, \"3\": {\"mcq\": \"multiple choice question\", \"options\": {\"a\": \"choice here\", \"b\": \"choice here\", \"c\": \"choice here\", \"d\": \"choice here\"}, \"correct\": \"correct answer\"}}',\n",
       " 'quiz': '{\\n    \"1\": {\\n        \"mcq\": \"What is the main difference between Polynomial Regression and Linear Regression?\",\\n        \"options\": {\\n            \"a\": \"Polynomial Regression is always non-linear, while Linear Regression is always linear.\",\\n            \"b\": \"Polynomial Regression is a non-linear function of the input x, while Linear Regression is linear on the coefficients.\",\\n            \"c\": \"Polynomial Regression can only handle one independent variable, while Linear Regression can handle multiple independent variables.\",\\n            \"d\": \"Polynomial Regression is more accurate than Linear Regression in all cases.\"\\n        },\\n        \"correct\": \"b\"\\n    },\\n    \"2\": {\\n        \"mcq\": \"Why do we not apply Feature Scaling in Polynomial Regression?\",\\n        \"options\": {\\n            \"a\": \"Feature Scaling is not necessary in Polynomial Regression.\",\\n            \"b\": \"The coefficients in Polynomial Regression can adapt their scale to put everything on the same scale.\",\\n            \"c\": \"Feature Scaling would make the model more complex and less accurate.\",\\n            \"d\": \"Feature Scaling is only applicable to non-linear models.\"\\n        },\\n        \"correct\": \"b\"\\n    },\\n    \"3\": {\\n        \"mcq\": \"When should you use Support Vector Regression (SVR)?\",\\n        \"options\": {\\n            \"a\": \"When dealing with linearly distributed data.\",\\n            \"b\": \"When a linear model like linear regression fits the data well.\",\\n            \"c\": \"When dealing with non-linear problems and data that is not linearly distributed.\",\\n            \"d\": \"When you want to use p-values for feature selection.\"\\n        },\\n        \"correct\": \"c\"\\n    },\\n    \"4\": {\\n        \"mcq\": \"What is the advantage of Random Forest Regression over Decision Trees?\",\\n        \"options\": {\\n            \"a\": \"Random Forests are more interpretable than Decision Trees.\",\\n            \"b\": \"Random Forests can give better predictive power than Decision Trees.\",\\n            \"c\": \"Decision Trees are faster to train than Random Forests.\",\\n            \"d\": \"Decision Trees can handle missing data better than Random Forests.\"\\n        },\\n        \"correct\": \"b\"\\n    },\\n    \"5\": {\\n        \"mcq\": \"How do you find the best degree in Polynomial Regression?\",\\n        \"options\": {\\n            \"a\": \"By selecting the highest degree possible to minimize error.\",\\n            \"b\": \"By finding the lowest root-mean-square error (RMSE) for the model.\",\\n            \"c\": \"By using p-values for feature selection.\",\\n            \"d\": \"By randomly selecting a degree and testing the model.\"\\n        },\\n        \"correct\": \"b\"\\n    },\\n    \"6\": {\\n        \"mcq\": \"What is the purpose of SVR in Machine Learning?\",\\n        \"options\": {\\n            \"a\": \"To handle linear regression problems.\",\\n            \"b\": \"To provide a more interpretable model than Decision Trees.\",\\n            \"c\": \"To address non-linear problems where data is not linearly distributed.\",\\n            \"d\": \"To perform feature extraction for dimensionality reduction.\"\\n        },\\n        \"correct\": \"c\"\\n    },\\n    \"7\": {\\n        \"mcq\": \"Why do we need to use \\'sc_Y.inverse_transform\\' in SVR?\",\\n        \"options\": {\\n            \"a\": \"To scale the features before training the SVR model.\",\\n            \"b\": \"To transform the target variable back to its original scale after prediction.\",\\n            \"c\": \"To apply a non-linear transformation to the data.\",\\n            \"d\": \"To remove outliers from the dataset.\"\\n        },\\n        \"correct\": \"b\"\\n    },\\n    \"8\": {\\n        \"mcq\": \"What is the Information Gain in Decision Trees?\",\\n        \"options\": {\\n            \"a\": \"It measures the disorder in a set resulting from a split.\",\\n            \"b\": \"It calculates by how much the Standard Deviation decreases after each split.\",\\n            \"c\": \"It is the same as the Entropy in Decision Trees.\",\\n            \"d\": \"It is the difference between the predicted value and the actual value.\"\\n        },\\n        \"correct\": \"b\"\\n    }\\n}',\n",
       " 'review': '{\\n    \"1\": {\\n        \"mcq\": \"What is the main difference between Polynomial Regression and Linear Regression?\",\\n        \"options\": {\\n            \"a\": \"Polynomial Regression is always non-linear, while Linear Regression is always linear\",\\n            \"b\": \"Polynomial Regression is linear on the coefficients, while Linear Regression is linear on the input variables\",\\n            \"c\": \"Polynomial Regression is a non-linear function of the input variables, while Linear Regression is a linear function\",\\n            \"d\": \"There is no difference between Polynomial Regression and Linear Regression\"\\n        },\\n        \"correct\": \"c\"\\n    },\\n    \"2\": {\\n        \"mcq\": \"Why do we not apply Feature Scaling in Polynomial Regression?\",\\n        \"options\": {\\n            \"a\": \"Because Feature Scaling is not necessary for Polynomial Regression\",\\n            \"b\": \"Because the coefficients in Polynomial Regression can adapt their scale\",\\n            \"c\": \"Because Polynomial Regression does not involve any scaling\",\\n            \"d\": \"Because Feature Scaling would lead to overfitting in Polynomial Regression\"\\n        },\\n        \"correct\": \"b\"\\n    },\\n    \"3\": {\\n        \"mcq\": \"When should you use Support Vector Regression (SVR)?\",\\n        \"options\": {\\n            \"a\": \"When dealing with linearly distributed data\",\\n            \"b\": \"When a linear model like Linear Regression fits the data well\",\\n            \"c\": \"When dealing with non-linearly distributed data\",\\n            \"d\": \"When you want to use p-values for feature selection\"\\n        },\\n        \"correct\": \"c\"\\n    },\\n    \"4\": {\\n        \"mcq\": \"What is the advantage of Random Forest Regression over Decision Trees?\",\\n        \"options\": {\\n            \"a\": \"Random Forests are more interpretable than Decision Trees\",\\n            \"b\": \"Random Forests can give better predictive power than Decision Trees\",\\n            \"c\": \"Decision Trees are faster than Random Forests\",\\n            \"d\": \"Decision Trees can handle more complex data than Random Forests\"\\n        },\\n        \"correct\": \"b\"\\n    },\\n    \"5\": {\\n        \"mcq\": \"How do you find the best degree in Polynomial Regression?\",\\n        \"options\": {\\n            \"a\": \"By selecting the highest degree possible\",\\n            \"b\": \"By finding the lowest root-mean-square error (RMSE)\",\\n            \"c\": \"By using p-values for feature selection\",\\n            \"d\": \"By fitting the model to the training data\"\\n        },\\n        \"correct\": \"b\"\\n    },\\n    \"6\": {\\n        \"mcq\": \"What is the purpose of SVR in Machine Learning?\",\\n        \"options\": {\\n            \"a\": \"To handle linearly distributed data\",\\n            \"b\": \"To provide a more interpretable model than Linear Regression\",\\n            \"c\": \"To address non-linear problems where Linear Regression does not fit well\",\\n            \"d\": \"To perform feature extraction on the data\"\\n        },\\n        \"correct\": \"c\"\\n    },\\n    \"7\": {\\n        \"mcq\": \"Why do we need to use \\'sc_Y.inverse_transform\\' in SVR?\",\\n        \"options\": {\\n            \"a\": \"To scale the features before fitting the SVR model\",\\n            \"b\": \"To transform the target variable back to its original scale after prediction\",\\n            \"c\": \"To apply feature selection on the target variable\",\\n            \"d\": \"To standardize the target variable for SVR\"\\n        },\\n        \"correct\": \"b\"\\n    },\\n    \"8\": {\\n        \"mcq\": \"What is the main form of finding a good fit in Decision Tree Regression?\",\\n        \"options\": {\\n            \"a\": \"By using p-values for feature selection\",\\n            \"b\": \"By plotting the model and visually inspecting the fit\",\\n            \"c\": \"By fitting the model to the training data\",\\n            \"d\": \"By applying cross-validation to evaluate the model\"\\n        },\\n        \"correct\": \"b\"\\n    }\\n}'}"
      ]
     },
     "execution_count": 653,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 654,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'{\\n    \"1\": {\\n        \"mcq\": \"What is the main difference between Polynomial Regression and Linear Regression?\",\\n        \"options\": {\\n            \"a\": \"Polynomial Regression is always non-linear, while Linear Regression is always linear.\",\\n            \"b\": \"Polynomial Regression is a non-linear function of the input x, while Linear Regression is linear on the coefficients.\",\\n            \"c\": \"Polynomial Regression can only handle one independent variable, while Linear Regression can handle multiple independent variables.\",\\n            \"d\": \"Polynomial Regression is more accurate than Linear Regression in all cases.\"\\n        },\\n        \"correct\": \"b\"\\n    },\\n    \"2\": {\\n        \"mcq\": \"Why do we not apply Feature Scaling in Polynomial Regression?\",\\n        \"options\": {\\n            \"a\": \"Feature Scaling is not necessary in Polynomial Regression.\",\\n            \"b\": \"The coefficients in Polynomial Regression can adapt their scale to put everything on the same scale.\",\\n            \"c\": \"Feature Scaling would make the model more complex and less accurate.\",\\n            \"d\": \"Feature Scaling is only applicable to non-linear models.\"\\n        },\\n        \"correct\": \"b\"\\n    },\\n    \"3\": {\\n        \"mcq\": \"When should you use Support Vector Regression (SVR)?\",\\n        \"options\": {\\n            \"a\": \"When dealing with linearly distributed data.\",\\n            \"b\": \"When a linear model like linear regression fits the data well.\",\\n            \"c\": \"When dealing with non-linear problems and data that is not linearly distributed.\",\\n            \"d\": \"When you want to use p-values for feature selection.\"\\n        },\\n        \"correct\": \"c\"\\n    },\\n    \"4\": {\\n        \"mcq\": \"What is the advantage of Random Forest Regression over Decision Trees?\",\\n        \"options\": {\\n            \"a\": \"Random Forests are more interpretable than Decision Trees.\",\\n            \"b\": \"Random Forests can give better predictive power than Decision Trees.\",\\n            \"c\": \"Decision Trees are faster to train than Random Forests.\",\\n            \"d\": \"Decision Trees can handle missing data better than Random Forests.\"\\n        },\\n        \"correct\": \"b\"\\n    },\\n    \"5\": {\\n        \"mcq\": \"How do you find the best degree in Polynomial Regression?\",\\n        \"options\": {\\n            \"a\": \"By selecting the highest degree possible to minimize error.\",\\n            \"b\": \"By finding the lowest root-mean-square error (RMSE) for the model.\",\\n            \"c\": \"By using p-values for feature selection.\",\\n            \"d\": \"By randomly selecting a degree and testing the model.\"\\n        },\\n        \"correct\": \"b\"\\n    },\\n    \"6\": {\\n        \"mcq\": \"What is the purpose of SVR in Machine Learning?\",\\n        \"options\": {\\n            \"a\": \"To handle linear regression problems.\",\\n            \"b\": \"To provide a more interpretable model than Decision Trees.\",\\n            \"c\": \"To address non-linear problems where data is not linearly distributed.\",\\n            \"d\": \"To perform feature extraction for dimensionality reduction.\"\\n        },\\n        \"correct\": \"c\"\\n    },\\n    \"7\": {\\n        \"mcq\": \"Why do we need to use \\'sc_Y.inverse_transform\\' in SVR?\",\\n        \"options\": {\\n            \"a\": \"To scale the features before training the SVR model.\",\\n            \"b\": \"To transform the target variable back to its original scale after prediction.\",\\n            \"c\": \"To apply a non-linear transformation to the data.\",\\n            \"d\": \"To remove outliers from the dataset.\"\\n        },\\n        \"correct\": \"b\"\\n    },\\n    \"8\": {\\n        \"mcq\": \"What is the Information Gain in Decision Trees?\",\\n        \"options\": {\\n            \"a\": \"It measures the disorder in a set resulting from a split.\",\\n            \"b\": \"It calculates by how much the Standard Deviation decreases after each split.\",\\n            \"c\": \"It is the same as the Entropy in Decision Trees.\",\\n            \"d\": \"It is the difference between the predicted value and the actual value.\"\\n        },\\n        \"correct\": \"b\"\\n    }\\n}'"
      ]
     },
     "execution_count": 654,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response.get(\"quiz\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 655,
   "metadata": {},
   "outputs": [],
   "source": [
    "quiz=response.get(\"quiz\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 656,
   "metadata": {},
   "outputs": [],
   "source": [
    "quiz=json.loads(quiz)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 657,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'{\\n    \"1\": {\\n        \"mcq\": \"What is the main difference between Polynomial Regression and Linear Regression?\",\\n        \"options\": {\\n            \"a\": \"Polynomial Regression is always non-linear, while Linear Regression is always linear\",\\n            \"b\": \"Polynomial Regression is linear on the coefficients, while Linear Regression is linear on the input variables\",\\n            \"c\": \"Polynomial Regression is a non-linear function of the input variables, while Linear Regression is a linear function\",\\n            \"d\": \"There is no difference between Polynomial Regression and Linear Regression\"\\n        },\\n        \"correct\": \"c\"\\n    },\\n    \"2\": {\\n        \"mcq\": \"Why do we not apply Feature Scaling in Polynomial Regression?\",\\n        \"options\": {\\n            \"a\": \"Because Feature Scaling is not necessary for Polynomial Regression\",\\n            \"b\": \"Because the coefficients in Polynomial Regression can adapt their scale\",\\n            \"c\": \"Because Polynomial Regression does not involve any scaling\",\\n            \"d\": \"Because Feature Scaling would lead to overfitting in Polynomial Regression\"\\n        },\\n        \"correct\": \"b\"\\n    },\\n    \"3\": {\\n        \"mcq\": \"When should you use Support Vector Regression (SVR)?\",\\n        \"options\": {\\n            \"a\": \"When dealing with linearly distributed data\",\\n            \"b\": \"When a linear model like Linear Regression fits the data well\",\\n            \"c\": \"When dealing with non-linearly distributed data\",\\n            \"d\": \"When you want to use p-values for feature selection\"\\n        },\\n        \"correct\": \"c\"\\n    },\\n    \"4\": {\\n        \"mcq\": \"What is the advantage of Random Forest Regression over Decision Trees?\",\\n        \"options\": {\\n            \"a\": \"Random Forests are more interpretable than Decision Trees\",\\n            \"b\": \"Random Forests can give better predictive power than Decision Trees\",\\n            \"c\": \"Decision Trees are faster than Random Forests\",\\n            \"d\": \"Decision Trees can handle more complex data than Random Forests\"\\n        },\\n        \"correct\": \"b\"\\n    },\\n    \"5\": {\\n        \"mcq\": \"How do you find the best degree in Polynomial Regression?\",\\n        \"options\": {\\n            \"a\": \"By selecting the highest degree possible\",\\n            \"b\": \"By finding the lowest root-mean-square error (RMSE)\",\\n            \"c\": \"By using p-values for feature selection\",\\n            \"d\": \"By fitting the model to the training data\"\\n        },\\n        \"correct\": \"b\"\\n    },\\n    \"6\": {\\n        \"mcq\": \"What is the purpose of SVR in Machine Learning?\",\\n        \"options\": {\\n            \"a\": \"To handle linearly distributed data\",\\n            \"b\": \"To provide a more interpretable model than Linear Regression\",\\n            \"c\": \"To address non-linear problems where Linear Regression does not fit well\",\\n            \"d\": \"To perform feature extraction on the data\"\\n        },\\n        \"correct\": \"c\"\\n    },\\n    \"7\": {\\n        \"mcq\": \"Why do we need to use \\'sc_Y.inverse_transform\\' in SVR?\",\\n        \"options\": {\\n            \"a\": \"To scale the features before fitting the SVR model\",\\n            \"b\": \"To transform the target variable back to its original scale after prediction\",\\n            \"c\": \"To apply feature selection on the target variable\",\\n            \"d\": \"To standardize the target variable for SVR\"\\n        },\\n        \"correct\": \"b\"\\n    },\\n    \"8\": {\\n        \"mcq\": \"What is the main form of finding a good fit in Decision Tree Regression?\",\\n        \"options\": {\\n            \"a\": \"By using p-values for feature selection\",\\n            \"b\": \"By plotting the model and visually inspecting the fit\",\\n            \"c\": \"By fitting the model to the training data\",\\n            \"d\": \"By applying cross-validation to evaluate the model\"\\n        },\\n        \"correct\": \"b\"\\n    }\\n}'}"
      ]
     },
     "execution_count": 657,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "{response[\"review\"]}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 658,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'text': 'Polynomial Regression\\n2.3.1 Polynomial Regression Intuition\\nIs Polynomial Regression a linear or non linear model?\\nThat depends on what you are referring to. Polynomial Regression is linear on the coefficients since we don’t\\nhave any power of the coefficients (all the coefficients are raised to the power of 1: b0, b1, ..., bn). However,\\nPolynomial Regression is a non linear function of the input x, since we have the inputs raised to several\\npowers: x (power 1), x\\n2\\n(power 2), ..., x\\nn (power n). That is how we can also see the Polynomial Regression\\nas a non linear model. Besides indeed, Polynomial Regression is appropriate when the data is non linearly\\ndistributed (meaning you can’t fit a straight line between y and x).\\nPage 8 of 51\\nMachine Learning A-Z Q&A\\n2.3.2 Polynomial Regression in Python\\nWhy didn’t we apply Feature Scaling in our Polynomial Regression model?\\nIt’s simply because, since y is a linear combination of x and x\\n2\\n, the coefficients can adapt their scale to put\\neverything on the same scale. For example if y takes values between 0 and 1, x takes values between 1 and\\n10 and x\\n2\\ntakes values between 1 and 100, then b1 can be multiplied by 0.1 and b2 can be multiplied by 0.01\\nso that y, b1x1 and b2x2 are all on the same scale.\\nHow do we find the best degree?\\nThe main form of finding a good fit is to plot the model and see what it looks like visually. You simply\\ntest several degrees and you see which one gives you the best fit. The other option is to find the lowest\\nroot-mean-square error (RMSE) for your model, but in that case be careful not to overfit the data.\\nWhy did we have to create a second linear regressor ’lin_reg_2’ ? Could we not have used\\n’lin_reg’ directly?\\nNo, because ’lin_reg’ is already fitted to X and y and now we want to fit a new linear model to Xpoly and\\ny. So we have to create a new regressor object. One must important that the fit method here finds the\\ncoefficient between the independent variables and the dependent variable. Therefore since ’lin_reg’ already\\ngot the coefficients of correlation between X and y, ’lin_reg_2’ has to be created to get some new coefficients\\nof correlations between Xpoly and y.\\n2.3.3 Polynomial Regression in R\\nIn R, I have to create manually the different columns for each polynomial feature? What if\\nthere are many polynomial features?\\nYou will very rarely have to create more than 4 polynomial features, otherwise you would get overfitting,\\nwhich must absolutely be avoided in Machine Learning. So even if you have to create them manually, it will\\nnever take too much of your time. Besides you can use the template.\\nHow do we find the best degree? (Asking this question again in case some students only do R\\nThe main form of finding a good fit is to plot the model and see what it looks like visually. You simply\\ntest several degrees and you see which one gives you the best fit. The other option is to find the lowest\\nroot-mean-square error (RMSE) for your model, but in that case be careful not to overfit the data.\\n2.4 SVR\\n2.4.1 SVR Intuition\\nWhen should I use SVR?\\nYou should use SVR if a linear model like linear regression doesn’t fit very well your data. This would mean\\nyou are dealing with a non linear problem, where your data is not linearly distributed. Therefore in that\\ncase SVR could be a much better solution.\\nI didn’t understand the Intuition Lecture. Am I in trouble?\\nNot at all. SVR is a pretty abstract model and besides it is not that commonly used. What you must rather\\nunderstand is the SVM model, which you will see in Part 3 - Classification. Then once you understand the\\nSVM model, you will get a better grasp of the SVR model, since the SVR is simply the SVM for Regression.\\nHowever we wanted to include SVR in this course to give you an extra option in your Machine Learning\\ntoolkit.\\nPage 9 of 51\\nMachine Learning A-Z Q&A\\n2.4.2 SVR in Python\\nWhy do we need to ’sc_Y.inverse_transform’ ?\\nWe need the inverse_transform method to go back to the original scale. Indeed we applied feature scaling\\nso we get this scale around 0 and if we make a prediction without inversing the scale we will get the\\nscaled predicted salary. And of course we want the real salary, not the scaled one, so we have to use\\n’sc_Y.inverse_transform’. Also what is important to understand is that ’transform’ and ’inverse_transform’\\nare paired methods.\\n2.4.3 SVR in R\\nWhy did we not apply feature scaling like we did explicitly in Python\\nThat’s because in svm() function of R, the values are automatically scaled.\\nCan we select the most significant variables thanks to the p-value like we did in R before?\\nYou couldn’t use p-value because SVR is not a linear model, and p-values apply only to linear models.\\nTherefore feature selection is out of the question. But you could do feature extraction, which you will see in\\nPart 9 - Dimensionality Reduction. That you can apply to Decision Trees, and it will reduce the number of\\nyour features.\\n2.5 Decision Tree Regression\\n2.5.1 Decision Tree Regression Intuition\\nHow does the algorithm split the data points?\\nIt uses reduction of standard deviation of the predictions. In other words, the standard deviation is decreased\\nright after a split. Hence, building a decision tree is all about finding the attribute that returns the highest\\nstandard deviation reduction (i.e., the most homogeneous branches).\\nWhat is the Information Gain and how does it work in Decision Trees?\\nThe Information Gain in Decision Tree Regression is exactly the Standard Deviation Reduction we are\\nlooking to reach. We calculate by how much the Standard Deviation decreases after each split. Because the\\nmore the Standard Deviation is decreased after a split, the more homogeneous the child nodes will be.\\nWhat is the Entropy and how does it work in Decision Trees?\\nThe Entropy measures the disorder in a set, here in a part resulting from a split. So the more homogeneous\\nis your data in a part, the lower will be the entropy. The more you have splits, the more you have chance\\nto find parts in which your data is homogeneous, and therefore the lower will be the entropy (close to 0) in\\nthese parts. However you might still find some nodes where the data is not homogeneous, and therefore the\\nentropy would not be that small.\\n2.5.2 Decision Tree Regression in Python\\nDoes a Decision Tree make much sense in 1D?\\nNot really, as we saw in the practical part of this section. In 1D (meaning one independent variable), the\\nDecision Tree clearly tends to overfit the data. The Decision Tree would be much more relevant in higher\\ndimension, but keep in mind that the implementation we made here in 1D would be exactly the same in\\nhigher dimension. Therefore you might want to keep that model in your toolkit in case you are dealing\\nwith a higher dimensional space. This will actually be the case in Part 3 - Classification, where we will use\\nDecision Tree for Classification in 2D, which you will see turns out to be more relevant.\\nPage 10 of 51\\nMachine Learning A-Z Q&A\\n2.5.3 Decision Tree Regression in R\\nWhy do we get different results between Python and R?\\nThe difference is likely due to the random split of data. If we did a cross-validation (see Part 10) on all the\\nmodels in both languages, then you would likely get a similar mean accuracy. That being said, we would\\nrecommend more using Python for Decision Trees since the model is slightly better implemented in Python.\\nIs the Decision Tree appropriate here?\\nHere in this example, we can clearly see that the fitting curve is a stair with large gaps in the discontinuities.\\nThat decision tree regression model is therefore not the most appropriate, and that is because we have only\\none independent variables taking discrete values. So what happened is that the prediction was made in the\\nlower part of the gap in Python, and made in the upper part of the gap in R. And since the gap is large, that\\nmakes a big difference. If we had much more observations, taking values with more continuity (like with a\\n0.1 step), the gaps would be smaller and therefore the predictions in Python and R far from each other.\\nCan we select the most significant variables thanks to the p-value like we did in R before?\\nYou couldn’t use p-value because Decision Tree is not a linear model, and p-values apply only to linear\\nmodels. Therefore feature selection is out of the question. But you could do feature extraction, which you\\nwill see in Part 9 - Dimensionality Reduction. That you can apply to Decision Trees, and it will reduce the\\nnumber of your features.\\n2.6 Random Forest Regression\\n2.6.1 Random Forest Regression Intuition\\nWhat is the advantage and drawback of Random Forests compared to Decision Trees? Advan\\x02tage: Random Forests can give you a better predictive power than Decision Trees.\\nDrawback: Decision Tree will give you more interpretability than Random Forests, because you can plot the\\ngraph of a Decision Tree to see the different splits leading to the prediction, as seen in the Intuition Lecture.\\nThat’s something you can’t do with Random Forests.\\nWhen to use Random Forest and when to use the other models?\\nThe best answer to that question is: try them all!\\nIndeed, thanks to the templates it will only take you 10 minutes to try all the models, which is very little\\ncompared to the time dedicated to the other parts of a data science project (like Data Preprocessing for\\nexample). So just don’t be scared to try all the regression models and compare the results (through cross\\nvalidation which we will see in Part \\n',\n",
       " 'number': 8,\n",
       " 'subject': 'Machine Learning',\n",
       " 'tone': 'simple',\n",
       " 'response_json': '{\"1\": {\"mcq\": \"multiple choice question\", \"options\": {\"a\": \"choice here\", \"b\": \"choice here\", \"c\": \"choice here\", \"d\": \"choice here\"}, \"correct\": \"correct answer\"}, \"2\": {\"mcq\": \"multiple choice question\", \"options\": {\"a\": \"choice here\", \"b\": \"choice here\", \"c\": \"choice here\", \"d\": \"choice here\"}, \"correct\": \"correct answer\"}, \"3\": {\"mcq\": \"multiple choice question\", \"options\": {\"a\": \"choice here\", \"b\": \"choice here\", \"c\": \"choice here\", \"d\": \"choice here\"}, \"correct\": \"correct answer\"}}',\n",
       " 'quiz': '{\\n    \"1\": {\\n        \"mcq\": \"What is the main difference between Polynomial Regression and Linear Regression?\",\\n        \"options\": {\\n            \"a\": \"Polynomial Regression is always non-linear, while Linear Regression is always linear.\",\\n            \"b\": \"Polynomial Regression is a non-linear function of the input x, while Linear Regression is linear on the coefficients.\",\\n            \"c\": \"Polynomial Regression can only handle one independent variable, while Linear Regression can handle multiple independent variables.\",\\n            \"d\": \"Polynomial Regression is more accurate than Linear Regression in all cases.\"\\n        },\\n        \"correct\": \"b\"\\n    },\\n    \"2\": {\\n        \"mcq\": \"Why do we not apply Feature Scaling in Polynomial Regression?\",\\n        \"options\": {\\n            \"a\": \"Feature Scaling is not necessary in Polynomial Regression.\",\\n            \"b\": \"The coefficients in Polynomial Regression can adapt their scale to put everything on the same scale.\",\\n            \"c\": \"Feature Scaling would make the model more complex and less accurate.\",\\n            \"d\": \"Feature Scaling is only applicable to non-linear models.\"\\n        },\\n        \"correct\": \"b\"\\n    },\\n    \"3\": {\\n        \"mcq\": \"When should you use Support Vector Regression (SVR)?\",\\n        \"options\": {\\n            \"a\": \"When dealing with linearly distributed data.\",\\n            \"b\": \"When a linear model like linear regression fits the data well.\",\\n            \"c\": \"When dealing with non-linear problems and data that is not linearly distributed.\",\\n            \"d\": \"When you want to use p-values for feature selection.\"\\n        },\\n        \"correct\": \"c\"\\n    },\\n    \"4\": {\\n        \"mcq\": \"What is the advantage of Random Forest Regression over Decision Trees?\",\\n        \"options\": {\\n            \"a\": \"Random Forests are more interpretable than Decision Trees.\",\\n            \"b\": \"Random Forests can give better predictive power than Decision Trees.\",\\n            \"c\": \"Decision Trees are faster to train than Random Forests.\",\\n            \"d\": \"Decision Trees can handle missing data better than Random Forests.\"\\n        },\\n        \"correct\": \"b\"\\n    },\\n    \"5\": {\\n        \"mcq\": \"How do you find the best degree in Polynomial Regression?\",\\n        \"options\": {\\n            \"a\": \"By selecting the highest degree possible to minimize error.\",\\n            \"b\": \"By finding the lowest root-mean-square error (RMSE) for the model.\",\\n            \"c\": \"By using p-values for feature selection.\",\\n            \"d\": \"By randomly selecting a degree and testing the model.\"\\n        },\\n        \"correct\": \"b\"\\n    },\\n    \"6\": {\\n        \"mcq\": \"What is the purpose of SVR in Machine Learning?\",\\n        \"options\": {\\n            \"a\": \"To handle linear regression problems.\",\\n            \"b\": \"To provide a more interpretable model than Decision Trees.\",\\n            \"c\": \"To address non-linear problems where data is not linearly distributed.\",\\n            \"d\": \"To perform feature extraction for dimensionality reduction.\"\\n        },\\n        \"correct\": \"c\"\\n    },\\n    \"7\": {\\n        \"mcq\": \"Why do we need to use \\'sc_Y.inverse_transform\\' in SVR?\",\\n        \"options\": {\\n            \"a\": \"To scale the features before training the SVR model.\",\\n            \"b\": \"To transform the target variable back to its original scale after prediction.\",\\n            \"c\": \"To apply a non-linear transformation to the data.\",\\n            \"d\": \"To remove outliers from the dataset.\"\\n        },\\n        \"correct\": \"b\"\\n    },\\n    \"8\": {\\n        \"mcq\": \"What is the Information Gain in Decision Trees?\",\\n        \"options\": {\\n            \"a\": \"It measures the disorder in a set resulting from a split.\",\\n            \"b\": \"It calculates by how much the Standard Deviation decreases after each split.\",\\n            \"c\": \"It is the same as the Entropy in Decision Trees.\",\\n            \"d\": \"It is the difference between the predicted value and the actual value.\"\\n        },\\n        \"correct\": \"b\"\\n    }\\n}',\n",
       " 'review': '{\\n    \"1\": {\\n        \"mcq\": \"What is the main difference between Polynomial Regression and Linear Regression?\",\\n        \"options\": {\\n            \"a\": \"Polynomial Regression is always non-linear, while Linear Regression is always linear\",\\n            \"b\": \"Polynomial Regression is linear on the coefficients, while Linear Regression is linear on the input variables\",\\n            \"c\": \"Polynomial Regression is a non-linear function of the input variables, while Linear Regression is a linear function\",\\n            \"d\": \"There is no difference between Polynomial Regression and Linear Regression\"\\n        },\\n        \"correct\": \"c\"\\n    },\\n    \"2\": {\\n        \"mcq\": \"Why do we not apply Feature Scaling in Polynomial Regression?\",\\n        \"options\": {\\n            \"a\": \"Because Feature Scaling is not necessary for Polynomial Regression\",\\n            \"b\": \"Because the coefficients in Polynomial Regression can adapt their scale\",\\n            \"c\": \"Because Polynomial Regression does not involve any scaling\",\\n            \"d\": \"Because Feature Scaling would lead to overfitting in Polynomial Regression\"\\n        },\\n        \"correct\": \"b\"\\n    },\\n    \"3\": {\\n        \"mcq\": \"When should you use Support Vector Regression (SVR)?\",\\n        \"options\": {\\n            \"a\": \"When dealing with linearly distributed data\",\\n            \"b\": \"When a linear model like Linear Regression fits the data well\",\\n            \"c\": \"When dealing with non-linearly distributed data\",\\n            \"d\": \"When you want to use p-values for feature selection\"\\n        },\\n        \"correct\": \"c\"\\n    },\\n    \"4\": {\\n        \"mcq\": \"What is the advantage of Random Forest Regression over Decision Trees?\",\\n        \"options\": {\\n            \"a\": \"Random Forests are more interpretable than Decision Trees\",\\n            \"b\": \"Random Forests can give better predictive power than Decision Trees\",\\n            \"c\": \"Decision Trees are faster than Random Forests\",\\n            \"d\": \"Decision Trees can handle more complex data than Random Forests\"\\n        },\\n        \"correct\": \"b\"\\n    },\\n    \"5\": {\\n        \"mcq\": \"How do you find the best degree in Polynomial Regression?\",\\n        \"options\": {\\n            \"a\": \"By selecting the highest degree possible\",\\n            \"b\": \"By finding the lowest root-mean-square error (RMSE)\",\\n            \"c\": \"By using p-values for feature selection\",\\n            \"d\": \"By fitting the model to the training data\"\\n        },\\n        \"correct\": \"b\"\\n    },\\n    \"6\": {\\n        \"mcq\": \"What is the purpose of SVR in Machine Learning?\",\\n        \"options\": {\\n            \"a\": \"To handle linearly distributed data\",\\n            \"b\": \"To provide a more interpretable model than Linear Regression\",\\n            \"c\": \"To address non-linear problems where Linear Regression does not fit well\",\\n            \"d\": \"To perform feature extraction on the data\"\\n        },\\n        \"correct\": \"c\"\\n    },\\n    \"7\": {\\n        \"mcq\": \"Why do we need to use \\'sc_Y.inverse_transform\\' in SVR?\",\\n        \"options\": {\\n            \"a\": \"To scale the features before fitting the SVR model\",\\n            \"b\": \"To transform the target variable back to its original scale after prediction\",\\n            \"c\": \"To apply feature selection on the target variable\",\\n            \"d\": \"To standardize the target variable for SVR\"\\n        },\\n        \"correct\": \"b\"\\n    },\\n    \"8\": {\\n        \"mcq\": \"What is the main form of finding a good fit in Decision Tree Regression?\",\\n        \"options\": {\\n            \"a\": \"By using p-values for feature selection\",\\n            \"b\": \"By plotting the model and visually inspecting the fit\",\\n            \"c\": \"By fitting the model to the training data\",\\n            \"d\": \"By applying cross-validation to evaluate the model\"\\n        },\\n        \"correct\": \"b\"\\n    }\\n}'}"
      ]
     },
     "execution_count": 658,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 659,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'1': {'mcq': 'What is the main difference between Polynomial Regression and Linear Regression?', 'options': {'a': 'Polynomial Regression is always non-linear, while Linear Regression is always linear.', 'b': 'Polynomial Regression is a non-linear function of the input x, while Linear Regression is linear on the coefficients.', 'c': 'Polynomial Regression can only handle one independent variable, while Linear Regression can handle multiple independent variables.', 'd': 'Polynomial Regression is more accurate than Linear Regression in all cases.'}, 'correct': 'b'}, '2': {'mcq': 'Why do we not apply Feature Scaling in Polynomial Regression?', 'options': {'a': 'Feature Scaling is not necessary in Polynomial Regression.', 'b': 'The coefficients in Polynomial Regression can adapt their scale to put everything on the same scale.', 'c': 'Feature Scaling would make the model more complex and less accurate.', 'd': 'Feature Scaling is only applicable to non-linear models.'}, 'correct': 'b'}, '3': {'mcq': 'When should you use Support Vector Regression (SVR)?', 'options': {'a': 'When dealing with linearly distributed data.', 'b': 'When a linear model like linear regression fits the data well.', 'c': 'When dealing with non-linear problems and data that is not linearly distributed.', 'd': 'When you want to use p-values for feature selection.'}, 'correct': 'c'}, '4': {'mcq': 'What is the advantage of Random Forest Regression over Decision Trees?', 'options': {'a': 'Random Forests are more interpretable than Decision Trees.', 'b': 'Random Forests can give better predictive power than Decision Trees.', 'c': 'Decision Trees are faster to train than Random Forests.', 'd': 'Decision Trees can handle missing data better than Random Forests.'}, 'correct': 'b'}, '5': {'mcq': 'How do you find the best degree in Polynomial Regression?', 'options': {'a': 'By selecting the highest degree possible to minimize error.', 'b': 'By finding the lowest root-mean-square error (RMSE) for the model.', 'c': 'By using p-values for feature selection.', 'd': 'By randomly selecting a degree and testing the model.'}, 'correct': 'b'}, '6': {'mcq': 'What is the purpose of SVR in Machine Learning?', 'options': {'a': 'To handle linear regression problems.', 'b': 'To provide a more interpretable model than Decision Trees.', 'c': 'To address non-linear problems where data is not linearly distributed.', 'd': 'To perform feature extraction for dimensionality reduction.'}, 'correct': 'c'}, '7': {'mcq': \"Why do we need to use 'sc_Y.inverse_transform' in SVR?\", 'options': {'a': 'To scale the features before training the SVR model.', 'b': 'To transform the target variable back to its original scale after prediction.', 'c': 'To apply a non-linear transformation to the data.', 'd': 'To remove outliers from the dataset.'}, 'correct': 'b'}, '8': {'mcq': 'What is the Information Gain in Decision Trees?', 'options': {'a': 'It measures the disorder in a set resulting from a split.', 'b': 'It calculates by how much the Standard Deviation decreases after each split.', 'c': 'It is the same as the Entropy in Decision Trees.', 'd': 'It is the difference between the predicted value and the actual value.'}, 'correct': 'b'}}\n",
      "<class 'dict'>\n"
     ]
    }
   ],
   "source": [
    "print(quiz)\n",
    "print(type(quiz))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 660,
   "metadata": {},
   "outputs": [],
   "source": [
    "quiz_table_data = []\n",
    "for key, value in quiz.items():\n",
    "    mcq = value[\"mcq\"]\n",
    "    options = \" | \".join(\n",
    "        [\n",
    "            f\"{option}: {option_value}\"\n",
    "            for option, option_value in value[\"options\"].items()\n",
    "            ]\n",
    "        )\n",
    "    correct = value[\"correct\"]\n",
    "    quiz_table_data.append({\"MCQ\": mcq, \"Choices\": options, \"Correct\": correct})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 661,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'MCQ': 'What is the main difference between Polynomial Regression and Linear Regression?',\n",
       "  'Choices': 'a: Polynomial Regression is always non-linear, while Linear Regression is always linear. | b: Polynomial Regression is a non-linear function of the input x, while Linear Regression is linear on the coefficients. | c: Polynomial Regression can only handle one independent variable, while Linear Regression can handle multiple independent variables. | d: Polynomial Regression is more accurate than Linear Regression in all cases.',\n",
       "  'Correct': 'b'},\n",
       " {'MCQ': 'Why do we not apply Feature Scaling in Polynomial Regression?',\n",
       "  'Choices': 'a: Feature Scaling is not necessary in Polynomial Regression. | b: The coefficients in Polynomial Regression can adapt their scale to put everything on the same scale. | c: Feature Scaling would make the model more complex and less accurate. | d: Feature Scaling is only applicable to non-linear models.',\n",
       "  'Correct': 'b'},\n",
       " {'MCQ': 'When should you use Support Vector Regression (SVR)?',\n",
       "  'Choices': 'a: When dealing with linearly distributed data. | b: When a linear model like linear regression fits the data well. | c: When dealing with non-linear problems and data that is not linearly distributed. | d: When you want to use p-values for feature selection.',\n",
       "  'Correct': 'c'},\n",
       " {'MCQ': 'What is the advantage of Random Forest Regression over Decision Trees?',\n",
       "  'Choices': 'a: Random Forests are more interpretable than Decision Trees. | b: Random Forests can give better predictive power than Decision Trees. | c: Decision Trees are faster to train than Random Forests. | d: Decision Trees can handle missing data better than Random Forests.',\n",
       "  'Correct': 'b'},\n",
       " {'MCQ': 'How do you find the best degree in Polynomial Regression?',\n",
       "  'Choices': 'a: By selecting the highest degree possible to minimize error. | b: By finding the lowest root-mean-square error (RMSE) for the model. | c: By using p-values for feature selection. | d: By randomly selecting a degree and testing the model.',\n",
       "  'Correct': 'b'},\n",
       " {'MCQ': 'What is the purpose of SVR in Machine Learning?',\n",
       "  'Choices': 'a: To handle linear regression problems. | b: To provide a more interpretable model than Decision Trees. | c: To address non-linear problems where data is not linearly distributed. | d: To perform feature extraction for dimensionality reduction.',\n",
       "  'Correct': 'c'},\n",
       " {'MCQ': \"Why do we need to use 'sc_Y.inverse_transform' in SVR?\",\n",
       "  'Choices': 'a: To scale the features before training the SVR model. | b: To transform the target variable back to its original scale after prediction. | c: To apply a non-linear transformation to the data. | d: To remove outliers from the dataset.',\n",
       "  'Correct': 'b'},\n",
       " {'MCQ': 'What is the Information Gain in Decision Trees?',\n",
       "  'Choices': 'a: It measures the disorder in a set resulting from a split. | b: It calculates by how much the Standard Deviation decreases after each split. | c: It is the same as the Entropy in Decision Trees. | d: It is the difference between the predicted value and the actual value.',\n",
       "  'Correct': 'b'}]"
      ]
     },
     "execution_count": 661,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "quiz_table_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 665,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "list"
      ]
     },
     "execution_count": 665,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(quiz_table_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 662,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>MCQ</th>\n",
       "      <th>Choices</th>\n",
       "      <th>Correct</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>What is the main difference between Polynomial...</td>\n",
       "      <td>a: Polynomial Regression is always non-linear,...</td>\n",
       "      <td>b</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Why do we not apply Feature Scaling in Polynom...</td>\n",
       "      <td>a: Feature Scaling is not necessary in Polynom...</td>\n",
       "      <td>b</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>When should you use Support Vector Regression ...</td>\n",
       "      <td>a: When dealing with linearly distributed data...</td>\n",
       "      <td>c</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>What is the advantage of Random Forest Regress...</td>\n",
       "      <td>a: Random Forests are more interpretable than ...</td>\n",
       "      <td>b</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>How do you find the best degree in Polynomial ...</td>\n",
       "      <td>a: By selecting the highest degree possible to...</td>\n",
       "      <td>b</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>What is the purpose of SVR in Machine Learning?</td>\n",
       "      <td>a: To handle linear regression problems. | b: ...</td>\n",
       "      <td>c</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Why do we need to use 'sc_Y.inverse_transform'...</td>\n",
       "      <td>a: To scale the features before training the S...</td>\n",
       "      <td>b</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>What is the Information Gain in Decision Trees?</td>\n",
       "      <td>a: It measures the disorder in a set resulting...</td>\n",
       "      <td>b</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 MCQ  \\\n",
       "0  What is the main difference between Polynomial...   \n",
       "1  Why do we not apply Feature Scaling in Polynom...   \n",
       "2  When should you use Support Vector Regression ...   \n",
       "3  What is the advantage of Random Forest Regress...   \n",
       "4  How do you find the best degree in Polynomial ...   \n",
       "5    What is the purpose of SVR in Machine Learning?   \n",
       "6  Why do we need to use 'sc_Y.inverse_transform'...   \n",
       "7    What is the Information Gain in Decision Trees?   \n",
       "\n",
       "                                             Choices Correct  \n",
       "0  a: Polynomial Regression is always non-linear,...       b  \n",
       "1  a: Feature Scaling is not necessary in Polynom...       b  \n",
       "2  a: When dealing with linearly distributed data...       c  \n",
       "3  a: Random Forests are more interpretable than ...       b  \n",
       "4  a: By selecting the highest degree possible to...       b  \n",
       "5  a: To handle linear regression problems. | b: ...       c  \n",
       "6  a: To scale the features before training the S...       b  \n",
       "7  a: It measures the disorder in a set resulting...       b  "
      ]
     },
     "execution_count": 662,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(quiz_table_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 663,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "list"
      ]
     },
     "execution_count": 663,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(quiz_table_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 666,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'dict' object has no attribute 'to_csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[666], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mquiz\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto_csv\u001b[49m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmachinelearning.csv\u001b[39m\u001b[38;5;124m\"\u001b[39m, index\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'dict' object has no attribute 'to_csv'"
     ]
    }
   ],
   "source": [
    "quiz.to_csv(\"machinelearning.csv\",index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 587,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'01_22_2025_16_22_53'"
      ]
     },
     "execution_count": 587,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datetime import datetime\n",
    "datetime.now().strftime('%m_%d_%Y_%H_%M_%S')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
